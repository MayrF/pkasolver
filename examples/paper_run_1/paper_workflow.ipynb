{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports and setup** \n",
    "Import packages, the config.py and architecture.py files and set global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import (\n",
    "    LEARNING_RATE,\n",
    "    NUM_EPOCHS,\n",
    "    TRAIN_SIZE,\n",
    "    FP_BITS,\n",
    "    FP_RADIUS,\n",
    "    list_node_features,\n",
    "    list_edge_features,\n",
    "    BATCH_SIZE,\n",
    "    NUM_ESTIMATORS,\n",
    "    num_node_features,\n",
    "    num_edge_features,\n",
    ")\n",
    "from pkasolver.constants import SEED, DEVICE\n",
    "from pkasolver.data import (\n",
    "    load_data,\n",
    "    preprocess_all,\n",
    "    train_validation_set_split,\n",
    "    make_stat_variables,\n",
    "    make_pyg_dataset_from_dataframe,\n",
    "    \n",
    ")\n",
    "from pkasolver.ml_architecture import (\n",
    "    gcn_full_training,\n",
    "    GCNPairSingleConv,\n",
    "    GCNPairTwoConv,\n",
    "    GCNProt,\n",
    "    GCNDeprot,\n",
    "    NNConvPair,\n",
    "    NNConvDeprot,\n",
    "    NNConvProt,\n",
    "\n",
    ")\n",
    "from pkasolver.chem import generate_morgan_fp_array, calculate_tanimoto_coefficient\n",
    "from pkasolver.ml import dataset_to_dataloader, test_ml_model, test_graph_model\n",
    "from pkasolver.stat import compute_stats\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6.25, 6.25)\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from rdkit.Chem import Draw, Mol\n",
    "random.seed(SEED)\n",
    "imgdir = \"images_and_tables\"\n",
    "os.makedirs(imgdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, title):\n",
    "    plt.plot(results['training-set'], label='training set')\n",
    "    plt.plot(results['validation-set'], label='validation set')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.ylim([0,4])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import raw data**\n",
    "Load data from sdf files, create conjugate molescules and store them in pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the directory for saving the run data exists \n",
    "# run_data stores pickle files of the dicts containing dataset-Dataframes, morgan fingerprints and pyG graphs)\n",
    "run_filename = \"run_data\"\n",
    "os.makedirs(f\"{run_filename}/\", exist_ok=True)\n",
    "\n",
    "# check if saved dictonary of dataset-Dataframes is available and if so, import it\n",
    "if os.path.isfile(f\"{run_filename}/dataset_dfs.pkl\"):\n",
    "    print(\"Loading data ...\")\n",
    "    with open(f\"{run_filename}/dataset_dfs.pkl\", \"rb\") as pickle_file:\n",
    "        dataset_dfs = pickle.load(pickle_file)\n",
    "\n",
    "# create DataFrames for each dataset, store them in a dictonary and save it as as .pkl file in in the run_data folder\n",
    "else:\n",
    "    print(\"Generating data ...\")\n",
    "    sdf_paths = load_data(\"../../data/Baltruschat/\")\n",
    "    dataset_dfs = preprocess_all(sdf_paths) # load all datasets in Dataframes and calculate protonated and deprotonated conjugates\n",
    "    dataset_dfs[\"train_split\"], dataset_dfs[\"val_split\"] = train_validation_set_split(\n",
    "        dataset_dfs[\"Training\"], TRAIN_SIZE, SEED\n",
    "    ) # take a copy of the \"Training\" dataset, shuffle and split it into train and validation datasets and store them as Dataframe in respective dict   \n",
    "\n",
    "    with open(f\"{run_filename}/dataset_dfs.pkl\", \"wb\") as pickle_file:\n",
    "        pickle.dump(dataset_dfs, pickle_file)\n",
    "\n",
    "# notification\n",
    "print(dataset_dfs.keys())\n",
    "display(dataset_dfs[\"train_split\"].head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Calulate fingerprint based data**\n",
    "Create Fingerprints, target-value objects and add best tanimoto similarities of fps form external validation set molecules with those of the train molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if saved dictonary of fingerprint data is available and if so, import it\n",
    "if os.path.isfile(f\"{run_filename}/fp_data.pkl\"):\n",
    "    with open(f\"{run_filename}/fp_data.pkl\", \"rb\") as pickle_file:\n",
    "        fp_data = pickle.load(pickle_file)\n",
    "\n",
    "# create fingerprint arrays (dim: num molecules x fp bits) for each dataset, store them in a dictonary and save it as as .pkl file in in the run_data folder\n",
    "else:\n",
    "    fp_data = {}\n",
    "    for name, df in dataset_dfs.items():\n",
    "        X_feat, y = make_stat_variables(df, [], [\"pKa\"])\n",
    "        X_prot = generate_morgan_fp_array(\n",
    "            df, \"protonated\", nBits=FP_BITS, radius=FP_RADIUS, useFeatures=True\n",
    "        )\n",
    "        X_deprot = generate_morgan_fp_array(\n",
    "            df, \"deprotonated\", nBits=FP_BITS, radius=FP_RADIUS, useFeatures=True\n",
    "        )\n",
    "        X = np.concatenate((X_prot, X_deprot), axis=1)\n",
    "        fp_data[f\"{name}\"] = {\"prot\": X_prot, \"deprot\": X_deprot, \"pair\": X, \"y\": y}\n",
    "\n",
    "    with open(f\"{run_filename}/fp_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump(fp_data, f)\n",
    "\n",
    "# add max tanimotosimilarity to the Dataframes of external test sets\n",
    "    train_name = \"train_split\"\n",
    "    val_name = \"val_split\"\n",
    "    training = \"Training\"\n",
    "    for name, dataset in fp_data.items():\n",
    "        if name in [train_name, val_name, training]:\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"calculating similarities for {name}\")\n",
    "            max_scores = []\n",
    "            for test_mol in dataset[\"prot\"]:\n",
    "                scores = []\n",
    "                for ref_mol in fp_data[train_name][\"prot\"]:\n",
    "                    scores.append(calculate_tanimoto_coefficient(test_mol, ref_mol))\n",
    "                max_scores.append(max(scores))\n",
    "            dataset_dfs[name][\"Similarity_max\"] = max_scores\n",
    "\n",
    "with open(f\"{run_filename}/dataset_dfs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dataset_dfs, f)\n",
    "\n",
    "# notification\n",
    "print(\"fp_data keys:\", fp_data.keys())\n",
    "print(f\"calculated/loaded fingerprint data successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Calculate graph data**\n",
    "Create graph data with node and edge features specified in the config file and prepare loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if saved dictonary of graph data is available and if so, import it\n",
    "if os.path.isfile(f\"{run_filename}/graph_data.pkl\"):\n",
    "    print(\"Loading data ...\")\n",
    "    with open(f\"{run_filename}/graph_data.pkl\", \"rb\") as f:\n",
    "        graph_data = pickle.load(f)\n",
    "\n",
    "# create list of 'PairData' graph objects for each dataset, store them in a dictonary and save it as as .pkl file in in the run_data folder\n",
    "else:\n",
    "    print(\"Generating data ...\")\n",
    "    graph_data = {}\n",
    "    for name, df in dataset_dfs.items():\n",
    "        print(f\"Generating data for: {name}\")\n",
    "        graph_data[name] = make_pyg_dataset_from_dataframe(\n",
    "            df, list_node_features, list_edge_features, paired=True\n",
    "        )\n",
    "    with open(f\"{run_filename}/graph_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump(graph_data, f)\n",
    "\n",
    "print(\"graph_data keys:\", graph_data.keys())\n",
    "\n",
    "# create an iterable loader object from the list of graph data of each dataset and store them in a dictonary\n",
    "loaders = {}\n",
    "for name, dataset in graph_data.items():\n",
    "    print(f\"Generating loader for {name}\")\n",
    "    if name == \"Training\":\n",
    "        print(\"Skipping unsplit Training dataset\")\n",
    "        continue\n",
    "    elif name == \"train_split\" or name == \"val_split\":\n",
    "        loaders[name] = dataset_to_dataloader(dataset, BATCH_SIZE, shuffle=True) # Shuffling is essential to avoid overfitting on particular batches\n",
    "    else:\n",
    "        loaders[name] = dataset_to_dataloader(dataset, BATCH_SIZE, shuffle=False) # Testsets must not be shuffled in order to be able to calculate per datapoint predcitons with all graph and baselinemodels in the analysis part\n",
    "\n",
    "# notification\n",
    "print(\"loaders keys:\", loaders.keys())\n",
    "print(f\"calculated/loaded graph data successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**show feature value range**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print all possible node feature values\n",
    "# values = [set() for i in range(len(list_node_features))]\n",
    "# for dataset in graph_data.values():\n",
    "#     for entry in dataset:\n",
    "#         for i, row in enumerate(entry.x_p.cpu().T):\n",
    "#             values[i] = values[i] | set(row.numpy())\n",
    "#         for i, row in enumerate(entry.x_d.cpu().T):\n",
    "#             values[i] = values[i] | set(row.numpy())\n",
    "# print(\"Node features:\")\n",
    "# for name, values in zip(list_node_features, values):\n",
    "#     x = list(values)\n",
    "#     x.sort()\n",
    "#     print(f\"{name}:{x}\")\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # print all possible edge feature values\n",
    "# values = [set() for i in range(len(list_edge_features))]\n",
    "# for dataset in graph_data.values():\n",
    "#     for entry in dataset:\n",
    "#         for i, row in enumerate(entry.edge_attr_p.cpu().T):\n",
    "#             values[i] = values[i] | set(row.numpy())\n",
    "#         for i, row in enumerate(entry.edge_attr_d.cpu().T):\n",
    "#             values[i] = values[i] | set(row.numpy())\n",
    "# print(\"Edge features:\")\n",
    "# for name, values in zip(list_edge_features, values):\n",
    "#     x = list(values)\n",
    "#     x.sort()\n",
    "#     print(f\"{name}:{x}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training of predictive models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **train baseline models**\n",
    "train all baseline models in protonated, deprotonated and pair mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    \"RFR\": RandomForestRegressor(\n",
    "        n_estimators=NUM_ESTIMATORS, random_state=SEED\n",
    "    ),  # Baltruschat n_estimatores = 1000\n",
    "    \"PLS\": PLSRegression(),\n",
    "}\n",
    "\n",
    "baseline_models = {}\n",
    "train_name = \"train_split\"\n",
    "val_name = \"val_split\"\n",
    "\n",
    "for model_name, model_template in models_dict.items():\n",
    "    baseline_models[model_name] = {}\n",
    "    for mode, X in fp_data[train_name].items():\n",
    "        if mode == \"y\":\n",
    "            continue\n",
    "        path = f\"models/baseline/{model_name}/{mode}/\"\n",
    "        if os.path.isfile(path + \"model.pkl\"):\n",
    "            with open(path + \"model.pkl\", \"rb\") as pickle_file:\n",
    "                baseline_models[model_name][mode] = pickle.load(pickle_file)\n",
    "        else:\n",
    "            print(f\"Training {model_name}_{mode}...\")\n",
    "            y = fp_data[train_name][\"y\"]\n",
    "            y_val = fp_data[val_name][\"y\"]\n",
    "            model = copy.deepcopy(model_template)\n",
    "            model.fit(X, y)\n",
    "            print(f\"{model_name}_{mode}: {model.score(fp_data[val_name][mode], y_val)}\")\n",
    "            baseline_models[model_name][mode] = model\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            with open(path + \"model.pkl\", \"wb\") as pickle_file:\n",
    "                pickle.dump(model, pickle_file)\n",
    "print(f\"trained/loaded baseline models successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train graph models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**\n",
    "train all graph models in protonated, deprotonated and pair mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43475/678930308.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training on {DEVICE}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         results = gcn_full_training(\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_split\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/shared/projects/pKa-prediction/anaconda3/envs/pkasolver/lib/python3.9/site-packages/pkasolver-0.1+54.g8daeda7.dirty-py3.9.egg/pkasolver/ml_architecture.py\u001b[0m in \u001b[0;36mgcn_full_training\u001b[0;34m(model, train_loader, val_loader, optimizer, path, NUM_EPOCHS)\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0mgcn_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcn_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/shared/projects/pKa-prediction/anaconda3/envs/pkasolver/lib/python3.9/site-packages/pkasolver-0.1+54.g8daeda7.dirty-py3.9.egg/pkasolver/ml_architecture.py\u001b[0m in \u001b[0;36mgcn_train\u001b[0;34m(model, loader, optimizer)\u001b[0m\n\u001b[1;32m    653\u001b[0m         )\n\u001b[1;32m    654\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute the loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Derive gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update parameters based on gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Clear gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/shared/projects/pKa-prediction/anaconda3/envs/pkasolver/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/shared/projects/pKa-prediction/anaconda3/envs/pkasolver/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = [('GCNPairSingleConv', GCNPairSingleConv), \n",
    "('GCNPairTwoConv', GCNPairTwoConv), \n",
    "('GCNProt', GCNProt), \n",
    "('GCNDeprot', GCNDeprot), \n",
    "('NNConvPair', NNConvPair), \n",
    "('NNConvDeprot', NNConvDeprot), \n",
    "('NNConvProt', NNConvProt )]\n",
    "\n",
    "for model_name, model_class in models:\n",
    "    path = f\"models/gcn/\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    pkl_file_name = f\"{path}/{model_name}.pkl\"\n",
    "    if os.path.isfile(pkl_file_name):\n",
    "        print('Attention: RELOADING model')\n",
    "        with open(pkl_file_name, \"rb\") as pickle_file:\n",
    "            model = pickle.load(pickle_file)\n",
    "    else:\n",
    "        model = model_class(num_node_features, num_edge_features)\n",
    "\n",
    "    if model.checkpoint[\"epoch\"] < NUM_EPOCHS:\n",
    "        model.to(device=DEVICE)\n",
    "        print(model.checkpoint[\"epoch\"])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))            \n",
    "        try:\n",
    "            optimizer.load_state_dict(model.checkpoint[\"optimizer_state\"])\n",
    "        except:\n",
    "            pass\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, patience=5, verbose=True\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f'Training {model_name} at epoch {model.checkpoint[\"epoch\"]} ...'\n",
    "        )\n",
    "        print(f'LR: {LEARNING_RATE}')\n",
    "        print(model_name)\n",
    "        print(model)\n",
    "        print(f'Training on {DEVICE}.')\n",
    "        results = gcn_full_training(\n",
    "            model.to(device=DEVICE),\n",
    "            loaders[\"train_split\"],\n",
    "            loaders[\"val_split\"],\n",
    "            optimizer,\n",
    "            pkl_file_name,\n",
    "            NUM_EPOCHS,\n",
    "        )\n",
    "\n",
    "        plot_results(results, f'{model_name}')\n",
    "        with open(f\"{path}/{model_name}.pkl\", \"wb\") as pickle_file:\n",
    "            pickle.dump(model.to(device='cpu'), pickle_file)\n",
    "print(f\"trained/loaded gcn models successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_models = {}\n",
    "for model_name, model_class in models:\n",
    "    path = f\"models/gcn/\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    pkl_file_name = f\"{path}/{model_name}.pkl\"\n",
    "    if os.path.isfile(pkl_file_name):\n",
    "        with open(pkl_file_name, 'rb') as pickle_file:\n",
    "            model = pickle.load(pickle_file)\n",
    "        model.to(device='cpu')\n",
    "        best_loss = max([x for x in model.checkpoint['best_states'].keys() if x < 4_000]) \n",
    "        model.load_state_dict(model.checkpoint['best_states'][best_loss][1])\n",
    "        loss = model.checkpoint['best_states'][best_loss][0]\n",
    "        print(f'{model_name},Epoch {best_loss}, Loss:{loss}')\n",
    "        graph_models[model_name] = model\n",
    "    else:\n",
    "        print(f'{path} not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions of baseline and graph models\n",
    "\n",
    "for i,dataset_name in enumerate(['Novartis', 'Literature', 'val_split']):\n",
    "\n",
    "    df_ml = test_ml_model(baseline_models, fp_data[dataset_name], fp_data[dataset_name]['y'],dataset_name)\n",
    "    df_gcn = test_graph_model(graph_models, loaders[dataset_name], dataset_name)\n",
    "    df= pd.concat([df_ml.drop(columns=['Dataset', 'pKa_true']), df_gcn],axis=1)\n",
    "    df= pd.concat([df_gcn],axis=1)\n",
    "    torch.cuda.empty_cache()\n",
    "    if i == 0:\n",
    "        df_res = df\n",
    "    else:\n",
    "        df_res = pd.concat([df_res,df])\n",
    "display(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions of baseline and graph models\n",
    "\n",
    "for i,test_set in enumerate(['Novartis', 'Literature', 'val_split']):\n",
    "\n",
    "    df_ml = test_ml_model(baseline_models, fp_data[test_set], fp_data[test_set]['y'],test_set)\n",
    "    df_gcn = test_graph_model(graph_models, loaders[test_set],test_set)\n",
    "    df= pd.concat([df_ml.drop(columns=['Dataset', 'pKa_true']),df_gcn],axis=1)\n",
    "    df= pd.concat([df_gcn],axis=1)\n",
    "    torch.cuda.empty_cache()\n",
    "    if i == 0:\n",
    "        df_res = df\n",
    "    else:\n",
    "        df_res = pd.concat([df_res,df])\n",
    "display(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= compute_stats(df_res, 'Dataset', 'pKa_true')\n",
    "test.to_csv(f'{imgdir}/stat_metrics.csv')\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot best model\n",
    "Plot the predictions of the best models for the validation and the two testsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df, x_column, y_column):\n",
    "    y = df[x_column]\n",
    "    y_hat = df[y_column]\n",
    "    stat_info = f\"\"\"\n",
    "        $r^2$ = {r2_score(y, y_hat): .2f}\n",
    "        $MAE$ = {mean_absolute_error(y, y_hat): .2f}\n",
    "        $RMSE$ = {mean_squared_error(y, y_hat): .2f}\n",
    "        \"\"\"\n",
    "        # r² = 0.78 [0.74, 0.81]\n",
    "    g = sns.JointGrid(data=df, x=x_column, y=y_column, xlim=(2,12), ylim=(2,12), height=3.125)\n",
    "    g.plot_joint(sns.regplot)\n",
    "    g.plot_marginals(sns.kdeplot, shade=True)\n",
    "    g.ax_joint.text(0, 1, stat_info, size='x-small', ha='left', va=\"top\", transform = g.ax_joint.transAxes)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper')\n",
    "d = df_res\n",
    "model='GCN_prot_edge'\n",
    "for dataset in d['Dataset'].unique():\n",
    "# for dataset, model in zip(['Novartis','Literature'],['GCN_pair_edge', 'GCN_deprot_no-edge']):\n",
    "    print(dataset)\n",
    "    g = plot_results(d[d['Dataset']== dataset], 'pKa_true', model)\n",
    "    g.set_axis_labels('pKa (true)', 'gcn_pair_edge')\n",
    "    plt.savefig(f'{imgdir}/regression_{dataset}_gcn_pair_edge.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(3.125,3.125))\n",
    "    sns.residplot(data=df_res[df_res['Dataset']==dataset],x='pKa_true', y=model, lowess=True)\n",
    "    plt.ylabel('Error')\n",
    "#     plt.title('gcn_pair_edge')\n",
    "    plt.savefig(f'{imgdir}/residuals_{dataset}_gcn_pair_edge.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCN training progress\n",
    "\n",
    "store training losses in DataFrame and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in list([['prot','edge'],['prot','no-edge']]):\n",
    "    df_prog=pd.DataFrame(graph_models[x][y].checkpoint['progress_table'])\n",
    "#     df_prog=pd.DataFrame(graph_models_cv[x][y][1].checkpoint['progress_table'])\n",
    "    #plot learning\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(3.125,3.125)\n",
    "    sns.lineplot(x='epoch',y='train_loss', label='Train Loss',data=df_prog,ax=ax)\n",
    "    sns.lineplot(x='epoch',y='test_loss',label='Validation Loss',data=df_prog,ax=ax)\n",
    "    ax.set_ylabel(\"Loss (MAE)\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_xlim(left=0, right=2000)\n",
    "    ax.set_ylim(top=1.75, bottom=0)\n",
    "#     plt.title(f'training progress of gcn_{x}_{y} model')\n",
    "    plt.savefig(f'{imgdir}/training_progress_gcn_{x}_{y}.pdf',bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Feature impact\n",
    "\n",
    "Importances of gcn_prot_edge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def boxplot(attr_data):\n",
    "#     plt.figure(figsize=(3.125,6.25))\n",
    "#     sns.boxplot(x=\"value\", y=\"variable\",\n",
    "#                 orient=\"h\",\n",
    "#                 data=attr_data,\n",
    "#                 whis=[0, 100], width=.6)\n",
    "\n",
    "#     # Add in points to show each observation\n",
    "#     sns.stripplot(x=\"value\", y=\"variable\",\n",
    "#                 orient=\"h\",\n",
    "#                 data=attr_data,\n",
    "#                 size=4, color=\".3\", linewidth=0)\n",
    "#     plt.ylabel('')\n",
    "\n",
    "# # types = ['prot','deprot','pair']\n",
    "# types = ['prot']\n",
    "# f_modes= ['no-edge','edge']\n",
    "# for data_type in types:\n",
    "#     for f_mode in f_modes: \n",
    "#         model = graph_models[data_type][f_mode]\n",
    "#         dataset = graph_data['train_split']\n",
    "#         ig = IntegratedGradients(model)\n",
    "#         attr_pre_df = calc_importances(ig, dataset, 100, list_node_features, list_edge_features) #adjust number of random samples\n",
    "\n",
    "#         attr_pre_df.iloc[:, 1:]=attr_pre_df.iloc[:, 1:].abs()\n",
    "#         attr_df=attr_pre_df.groupby('ID').max()\n",
    "#         attr_data = pd.melt(attr_df)\n",
    "        \n",
    "#         if data_type== 'pair':\n",
    "#             split = len(attr_data.variable.unique())//2\n",
    "#             attr_data1 = pd.melt(attr_df.iloc[:,0:split])\n",
    "#             attr_data2 = pd.melt(attr_df.iloc[:,split:])\n",
    "        \n",
    "#             boxplot(attr_data1)\n",
    "# #             plt.title(f'gcn_{data_type}_1_{f_mode}')\n",
    "#             plt.savefig(f'{imgdir}/importances_{data_type}_1_{f_mode}.pdf', bbox_inches='tight')\n",
    "#             plt.show()\n",
    "#             boxplot(attr_data2)\n",
    "# #             plt.title(f'gcn_{data_type}_2_{f_mode}')\n",
    "#             plt.savefig(f'{imgdir}/importances_{data_type}_2_{f_mode}.pdf', bbox_inches='tight')\n",
    "#             plt.show()\n",
    "            \n",
    "#         else:\n",
    "#             boxplot(attr_data)\n",
    "# #             plt.title(f'gcn_{data_type}_{f_mode}')\n",
    "#             plt.savefig(f'{imgdir}/importances_{data_type}_{f_mode}.pdf', bbox_inches='tight')\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics by tanimoto similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1= pd.concat([df_res[['Dataset', 'pKa_true']],df_res.loc[:,(df_res.columns.str.startswith('GCN'))]],axis=1)\n",
    "for data_set in ['Novartis', 'Literature']:\n",
    "    df = x1[x1['Dataset']==data_set].copy()\n",
    "    df['similarity'] = dataset_dfs[data_set].loc[:,'Similarity_max']\n",
    "    df.sort_values(by=['similarity'], inplace=True)\n",
    "\n",
    "    res=[]\n",
    "    tanimoto=[]\n",
    "    maximum=0\n",
    "\n",
    "    for i in range(2,len(df)):\n",
    "        df2 = df.iloc[:i,:]\n",
    "        new_maximum = df2['similarity'].max()\n",
    "        if new_maximum <= maximum:\n",
    "            tanimoto[-1]= new_maximum\n",
    "            res[-1]= compute_stats(df2, 'Dataset', 'pKa_true',col_exclude=['similarity'])\n",
    "        else: \n",
    "            tanimoto.append(new_maximum)\n",
    "            res.append(compute_stats(df2, 'Dataset', 'pKa_true', col_exclude=['similarity']))\n",
    "        maximum=new_maximum\n",
    "    result = pd.concat((res), keys=tanimoto)\n",
    "    result\n",
    "\n",
    "    # X=result['Novartis'].loc[(slice(None),'pKa_gcn_prot_edge'),:].reset_index()\n",
    "    X=result[data_set].reset_index()\n",
    "    plt.figure(figsize=(6.25,4))\n",
    "    ax = sns.scatterplot(x='level_0', y=\"RMSE\", hue='level_1', palette='colorblind', data=X)\n",
    "    legend = ax.legend()\n",
    "    legend.texts[0] = ''\n",
    "    plt.xlabel('Similarity')\n",
    "    plt.savefig(f'{imgdir}/RMSE_sim_{data_set}.pdf')\n",
    "    \n",
    "    x2= x1\n",
    "    df = x2[x2['Dataset']==data_set].copy()\n",
    "    df['similarity'] = dataset_dfs[data_set].loc[:,'Similarity_max']\n",
    "    df.sort_values(by=['similarity'], inplace=True)\n",
    "    \n",
    "    df = df.loc[:,('pKa_true','GCN_pair_edge','similarity')]\n",
    "\n",
    "    df['Error']= df['GCN_pair_edge']-df['pKa_true']\n",
    "\n",
    "    sims=[]\n",
    "    step_size=0.15\n",
    "    for sim in df['similarity']:\n",
    "        x=1\n",
    "        while sim < x:\n",
    "            x+= -step_size\n",
    "        sims.append(f'< {round(np.clip(x+step_size,0,1),3)}')\n",
    "    df['group']=sims            \n",
    "\n",
    "    plt.figure(figsize=(6.25/2,2.5))\n",
    "    sns.boxplot(x=\"group\", y=\"Error\",\n",
    "                orient=\"v\",\n",
    "\n",
    "                whis=[0, 100], width=.6,\n",
    "                data=df\n",
    "               )\n",
    "    # Add in points to show each observation\n",
    "    sns.stripplot(x=\"group\", y=\"Error\",\n",
    "                orient=\"v\",\n",
    "                data=df,\n",
    "                  size=4, color=\".3\", linewidth=0)\n",
    "    plt.xlabel('Similarity')\n",
    "    plt.ylabel('Error [pKa units]')\n",
    "    plt.savefig(f'{imgdir}/error_sim_bloxplot_{data_set}.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Outliers top list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_molecule(m):\n",
    "  copy = Mol(m)\n",
    "  copy.Compute2DCoords(clearConfs=True)\n",
    "  return copy\n",
    "\n",
    "def group_by_range(series, range_list, decimal=1):\n",
    "    group_labels=[]\n",
    "    for x in series:\n",
    "        i=0\n",
    "        while x > range_list[i]:\n",
    "            i+=1\n",
    "        group_labels.append(round(range_list[i],decimal))\n",
    "    return group_labels \n",
    "\n",
    "data_set=['Novartis', 'Literature']\n",
    "best=[True, False]\n",
    "for data_set in data_set:\n",
    "    trues= df_res[df_res.Dataset==data_set].pKa_true\n",
    "    preds= df_res[df_res.Dataset==data_set].GCN_pair_edge\n",
    "    diffs = []\n",
    "    errors = []\n",
    "    for pred, true in zip(preds,trues):\n",
    "        diffs.append(pred-true)\n",
    "        errors.append(abs(pred-true))\n",
    "    res = pd.concat((pd.DataFrame({'differences':diffs}),pd.DataFrame({'errors':errors}), dataset_dfs['Novartis'].loc[:,('pKa','marvin_atom','protonated', 'deprotonated', 'ID','Similarity_max')]),axis=1)\n",
    "    \n",
    "    res_e=res.loc[:, ('errors','pKa','Similarity_max')]\n",
    "    res_e['pKa']=group_by_range(res_e['pKa'],list(range(2,14,2)))\n",
    "    res_e['Similarity']=group_by_range(res['Similarity_max'],np.arange(0.0,1.2,0.2))\n",
    "    res_e=res_e.loc[:, ('errors','pKa','Similarity')]\n",
    "    res_e=res_e.groupby(['Similarity','pKa']).mean().unstack()\n",
    "#     display(res_e)\n",
    "    \n",
    "    plt.figure(figsize=(3.125,2.5))\n",
    "    sns.heatmap(res_e['errors'], cmap='RdYlGn_r', vmin=0,vmax=1.50)\n",
    "    plt.savefig(f'{imgdir}/error_heatmap_{data_set}.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    for mod in best:    \n",
    "        res.sort_values(by=['errors'], inplace=True, ascending=mod)\n",
    "        num=6\n",
    "        img=Draw.MolsToGridImage(res.protonated[:num].map(get_2d_molecule),\n",
    "                                 molsPerRow=3,\n",
    "                                 subImgSize=(400,350),\n",
    "                                 useSVG=True,\n",
    "                                 highlightAtomLists=[[int(i)] for i in res.marvin_atom[:num]],\n",
    "                                 legends=[f\"error:  {round(x[1],2)}, pKa:  {x[0]}, sim: {x[2]:.3f}\" for x in zip(res.pKa[:num],res.differences[:num], res.Similarity_max[:num])])\n",
    "\n",
    "        display(img)\n",
    "        name_dict={True:'best',False:'outlier'}\n",
    "        with open(f'{imgdir}/grid_{data_set}_{name_dict[mod]}.svg', 'w') as f:\n",
    "            f.write(img.data)\n",
    "    # res.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2238d854a1993a20f5a2e769920d74e68befec1db0b55829c688fae57b1385d3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pkasolver': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
