{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports and setup** \n",
    "Import packages, the config.py and architecture.py files and set global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import (\n",
    "    LEARNING_RATE,\n",
    "    NUM_EPOCHS,\n",
    "    TRAIN_SIZE,\n",
    "    FP_BITS,\n",
    "    FP_RADIUS,\n",
    "    node_feat_list,\n",
    "    edge_feat_list,\n",
    "    BATCH_SIZE,\n",
    "    NUM_ESTIMATORS,\n",
    ")\n",
    "from pkasolver.constants import SEED, DEVICE, node_feat_values, edge_feat_values\n",
    "from pkasolver.data import (\n",
    "    load_data,\n",
    "    preprocess_all,\n",
    "    train_validation_set_split,\n",
    "    make_stat_variables,\n",
    "    make_pyg_dataset_from_dataframe,\n",
    "    calculate_nr_of_features,\n",
    "    \n",
    ")\n",
    "from pkasolver.ml_architecture import (\n",
    "    gcn_full_training,\n",
    "    GCNPairSingleConv,\n",
    "    GCNPairTwoConv,\n",
    "    GCNProt,\n",
    "    GCNDeprot,\n",
    "    NNConvPair,\n",
    "    NNConvDeprot,\n",
    "    NNConvProt,\n",
    "    GATPair, \n",
    "    GATProt, \n",
    "    GINPairV1,\n",
    "    GINPairV2, \n",
    "    GINProt\n",
    "\n",
    ")\n",
    "from pkasolver.chem import generate_morgan_fp_array, calculate_tanimoto_coefficient\n",
    "from pkasolver.ml import dataset_to_dataloader, test_ml_model, test_graph_model\n",
    "from pkasolver.stat import compute_stats, compute_kl_divergence, calc_rmse\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6.25, 6.25)\n",
    "# plt.rcParams[\"figure.figsize\"] = (12.5, 12.5)\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from rdkit.Chem import Draw, Mol\n",
    "random.seed(SEED)\n",
    "imgdir = \"images_and_tables\"\n",
    "os.makedirs(imgdir, exist_ok=True)\n",
    "\n",
    "num_node_features = calculate_nr_of_features(node_feat_list)\n",
    "num_edge_features = calculate_nr_of_features(edge_feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, title):\n",
    "    plt.plot(results['training-set'], label='training set')\n",
    "    plt.plot(results['validation-set'], label='validation set')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.ylim([0,4])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import raw data**\n",
    "Load data from sdf files, create conjugate molescules and store them in pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the directory for saving the run data exists \n",
    "# run_data stores pickle files of the dicts containing dataset-Dataframes, morgan fingerprints and pyG graphs)\n",
    "run_filename = \"run_data\"\n",
    "os.makedirs(f\"{run_filename}/\", exist_ok=True)\n",
    "\n",
    "# check if saved dictonary of dataset-Dataframes is available and if so, import it\n",
    "if os.path.isfile(f\"{run_filename}/dataset_dfs.pkl\"):\n",
    "    print(\"Loading data ...\")\n",
    "    with open(f\"{run_filename}/dataset_dfs.pkl\", \"rb\") as pickle_file:\n",
    "        dataset_dfs = pickle.load(pickle_file)\n",
    "\n",
    "# create DataFrames for each dataset, store them in a dictonary and save it as as .pkl file in in the run_data folder\n",
    "else:\n",
    "    print(\"Generating data ...\")\n",
    "    sdf_paths = load_data(\"../../data/Baltruschat/\")\n",
    "    dataset_dfs = preprocess_all(sdf_paths) # load all datasets in Dataframes and calculate protonated and deprotonated conjugates\n",
    "    dataset_dfs[\"train_split\"], dataset_dfs[\"val_split\"] = train_validation_set_split(\n",
    "        dataset_dfs[\"Training\"], TRAIN_SIZE, SEED\n",
    "    ) # take a copy of the \"Training\" dataset, shuffle and split it into train and validation datasets and store them as Dataframe in respective dict   \n",
    "\n",
    "    with open(f\"{run_filename}/dataset_dfs.pkl\", \"wb\") as pickle_file:\n",
    "        pickle.dump(dataset_dfs, pickle_file)\n",
    "\n",
    "# notification\n",
    "print(dataset_dfs.keys())\n",
    "display(dataset_dfs[\"train_split\"].head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Calulate fingerprint based data**\n",
    "Create Fingerprints, target-value objects and add best tanimoto similarities of fps form external validation set molecules with those of the train molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if saved dictonary of fingerprint data is available and if so, import it\n",
    "if os.path.isfile(f\"{run_filename}/fp_data.pkl\"):\n",
    "    with open(f\"{run_filename}/fp_data.pkl\", \"rb\") as pickle_file:\n",
    "        fp_data = pickle.load(pickle_file)\n",
    "\n",
    "# create fingerprint arrays (dim: num molecules x fp bits) for each dataset, store them in a dictonary and save it as as .pkl file in in the run_data folder\n",
    "else:\n",
    "    fp_data = {}\n",
    "    for name, df in dataset_dfs.items():\n",
    "        X_feat, y = make_stat_variables(df, [], [\"pKa\"])\n",
    "        X_prot = generate_morgan_fp_array(\n",
    "            df, \"protonated\", nBits=FP_BITS, radius=FP_RADIUS, useFeatures=True\n",
    "        )\n",
    "        X_deprot = generate_morgan_fp_array(\n",
    "            df, \"deprotonated\", nBits=FP_BITS, radius=FP_RADIUS, useFeatures=True\n",
    "        )\n",
    "        X = np.concatenate((X_prot, X_deprot), axis=1)\n",
    "        fp_data[f\"{name}\"] = {\"prot\": X_prot, \"deprot\": X_deprot, \"pair\": X, \"y\": y}\n",
    "\n",
    "    with open(f\"{run_filename}/fp_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump(fp_data, f)\n",
    "\n",
    "# add max tanimotosimilarity to the Dataframes of external test sets\n",
    "    train_name = \"train_split\"\n",
    "    val_name = \"val_split\"\n",
    "    training = \"Training\"\n",
    "    for name, dataset in fp_data.items():\n",
    "        if name in [train_name, val_name, training]:\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"calculating similarities for {name}\")\n",
    "            max_scores = []\n",
    "            for test_mol in dataset[\"prot\"]:\n",
    "                scores = []\n",
    "                for ref_mol in fp_data[train_name][\"prot\"]:\n",
    "                    scores.append(calculate_tanimoto_coefficient(test_mol, ref_mol))\n",
    "                max_scores.append(max(scores))\n",
    "            dataset_dfs[name][\"Similarity_max\"] = max_scores\n",
    "\n",
    "with open(f\"{run_filename}/dataset_dfs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dataset_dfs, f)\n",
    "\n",
    "# notification\n",
    "print(\"fp_data keys:\", fp_data.keys())\n",
    "print(f\"calculated/loaded fingerprint data successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Calculate graph data**\n",
    "Create graph data with node and edge features specified in the config file and prepare loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if saved dictonary of graph data is available and if so, import it\n",
    "if os.path.isfile(f\"{run_filename}/graph_data.pkl\"):\n",
    "    print(\"Loading data ...\")\n",
    "    with open(f\"{run_filename}/graph_data.pkl\", \"rb\") as f:\n",
    "        graph_data = pickle.load(f)\n",
    "\n",
    "# create list of 'PairData' graph objects for each dataset, store them in a dictonary and save it as as .pkl file in in the run_data folder\n",
    "else:\n",
    "    print(\"Generating data ...\")\n",
    "    graph_data = {}\n",
    "    for name, df in dataset_dfs.items():\n",
    "        print(f\"Generating data for: {name}\")\n",
    "        graph_data[name] = make_pyg_dataset_from_dataframe(\n",
    "            df, node_feat_list, edge_feat_list, paired=True\n",
    "        )\n",
    "    with open(f\"{run_filename}/graph_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump(graph_data, f)\n",
    "\n",
    "print(\"graph_data keys:\", graph_data.keys())\n",
    "\n",
    "# create an iterable loader object from the list of graph data of each dataset and store them in a dictonary\n",
    "loaders = {}\n",
    "for name, dataset in graph_data.items():\n",
    "    print(f\"Generating loader for {name}\")\n",
    "    if name == \"Training\":\n",
    "        print(\"Skipping unsplit Training dataset\")\n",
    "        continue\n",
    "    elif name == \"train_split\" or name == \"val_split\":\n",
    "        loaders[name] = dataset_to_dataloader(dataset, BATCH_SIZE, shuffle=True) # Shuffling is essential to avoid overfitting on particular batches\n",
    "    else:\n",
    "        loaders[name] = dataset_to_dataloader(dataset, BATCH_SIZE, shuffle=False) # Testsets must not be shuffled in order to be able to calculate per datapoint predcitons with all graph and baselinemodels in the analysis part\n",
    "\n",
    "# notification\n",
    "print(\"loaders keys:\", loaders.keys())\n",
    "print(f\"calculated/loaded graph data successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import node_feat_values\n",
    "pd.set_option('display.max_columns', None)\n",
    "def color_positive_red(val):\n",
    "    color = 'red' if val > 0 else 'black'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "lst = graph_data['Novartis'][1].x_p\n",
    "\n",
    "\n",
    "node_feat_names = []\n",
    "for feat in node_feat_list:\n",
    "    values = node_feat_values[feat]\n",
    "    for value in values:\n",
    "        node_feat_names.append(f'{feat} = {value}')\n",
    "\n",
    "df = pd.DataFrame(lst, columns= node_feat_names, dtype=int)\n",
    "df.style.applymap(color_positive_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data['Novartis'][1].x_p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**show feature value range**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print all possible node feature values\n",
    "# values = [set() for i in range(len(node_feat_list))]\n",
    "# for dataset in graph_data.values():\n",
    "#     for entry in dataset:\n",
    "#         for i, row in enumerate(entry.x_p.cpu().T):\n",
    "#             values[i] = values[i] | set(row.numpy())\n",
    "#         for i, row in enumerate(entry.x_d.cpu().T):\n",
    "#             values[i] = values[i] | set(row.numpy())\n",
    "# print(\"Node features:\")\n",
    "# for name, values in zip(node_feat_list, values):\n",
    "#     x = list(values)\n",
    "#     x.sort()\n",
    "#     print(f\"{name}:{x}\")\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # print all possible edge feature values\n",
    "# values = [set() for i in range(len(edge_feat_list))]\n",
    "# for dataset in graph_data.values():\n",
    "#     for entry in dataset:\n",
    "#         for i, row in enumerate(entry.edge_attr_p.cpu().T):\n",
    "#             values[i] = values[i] | set(row.numpy())\n",
    "#         for i, row in enumerate(entry.edge_attr_d.cpu().T):\n",
    "#             values[i] = values[i] | set(row.numpy())\n",
    "# print(\"Edge features:\")\n",
    "# for name, values in zip(edge_feat_list, values):\n",
    "#     x = list(values)\n",
    "#     x.sort()\n",
    "#     print(f\"{name}:{x}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training of predictive models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **train baseline models**\n",
    "train all baseline models in protonated, deprotonated and pair mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    \"RFR\": RandomForestRegressor(\n",
    "        n_estimators=NUM_ESTIMATORS, random_state=SEED\n",
    "    ),  # Baltruschat n_estimatores = 1000\n",
    "    \"PLS\": PLSRegression(),\n",
    "}\n",
    "\n",
    "baseline_models = {}\n",
    "train_name = \"train_split\"\n",
    "val_name = \"val_split\"\n",
    "\n",
    "for model_name, model_template in models_dict.items():\n",
    "    baseline_models[model_name] = {}\n",
    "    for mode, X in fp_data[train_name].items():\n",
    "        if mode == \"y\":\n",
    "            continue\n",
    "        path = f\"models/baseline/{model_name}/{mode}/\"\n",
    "        if os.path.isfile(path + \"model.pkl\"):\n",
    "            with open(path + \"model.pkl\", \"rb\") as pickle_file:\n",
    "                baseline_models[model_name][mode] = pickle.load(pickle_file)\n",
    "        else:\n",
    "            print(f\"Training {model_name}_{mode}...\")\n",
    "            y = fp_data[train_name][\"y\"]\n",
    "            y_val = fp_data[val_name][\"y\"]\n",
    "            model = copy.deepcopy(model_template)\n",
    "            model.fit(X, y)\n",
    "            print(f\"{model_name}_{mode}: {model.score(fp_data[val_name][mode], y_val)}\")\n",
    "            baseline_models[model_name][mode] = model\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            with open(path + \"model.pkl\", \"wb\") as pickle_file:\n",
    "                pickle.dump(model, pickle_file)\n",
    "print(f\"trained/loaded baseline models successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train graph models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**\n",
    "train all graph models in protonated, deprotonated and pair mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkasolver.ml_architecture import GINProt, GINPairV1,GINPairV2, AttentiveProt\n",
    "NUM_EPOCHS = 40\n",
    "\n",
    "models = [('GCNPairSingleConv', GCNPairSingleConv), \n",
    "('GCNPairTwoConv', GCNPairTwoConv), \n",
    "('GCNProt', GCNProt), \n",
    "('GCNDeprot', GCNDeprot), \n",
    "('NNConvPair', NNConvPair), \n",
    "('NNConvDeprot', NNConvDeprot), \n",
    "('NNConvProt', NNConvProt )]\n",
    "\n",
    "# models = [(\"AttentiveProt\", AttentiveProt), (\"GINPairV2\", GINPairV2), (\"GINPairV1\", GINPairV1), ]\n",
    "\n",
    "\n",
    "for model_name, model_class in models:\n",
    "    path = f\"models/gcn/\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    pkl_file_name = f\"{path}/{model_name}.pkl\"\n",
    "    if os.path.isfile(pkl_file_name):\n",
    "        print('Attention: RELOADING model')\n",
    "        with open(pkl_file_name, \"rb\") as pickle_file:\n",
    "            model = pickle.load(pickle_file)\n",
    "    else:\n",
    "        model = model_class(num_node_features, num_edge_features, hidden_channels = 32)\n",
    "\n",
    "    if model.checkpoint[\"epoch\"] < NUM_EPOCHS:\n",
    "        model.to(device=DEVICE)\n",
    "        print(model.checkpoint[\"epoch\"])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))            \n",
    "        try:\n",
    "            optimizer.load_state_dict(model.checkpoint[\"optimizer_state\"])\n",
    "        except:\n",
    "            pass\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, patience=5, verbose=True\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f'Training {model_name} at epoch {model.checkpoint[\"epoch\"]} ...'\n",
    "        )\n",
    "        print(f'LR: {LEARNING_RATE}')\n",
    "        print(model_name)\n",
    "        print(model)\n",
    "        print(f'Training on {DEVICE}.')\n",
    "        results = gcn_full_training(\n",
    "            model.to(device=DEVICE),\n",
    "            loaders[\"train_split\"],\n",
    "            loaders[\"val_split\"],\n",
    "            optimizer,\n",
    "            pkl_file_name,\n",
    "            NUM_EPOCHS,\n",
    "        )\n",
    "\n",
    "        plot_results(results, f'{model_name}')\n",
    "        with open(f\"{path}/{model_name}.pkl\", \"wb\") as pickle_file:\n",
    "            pickle.dump(model.to(device='cpu'), pickle_file)\n",
    "print(f\"trained/loaded gcn models successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_models = {}\n",
    "for model_name, model_class in models:\n",
    "    path = f\"models/gcn/\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    pkl_file_name = f\"{path}/{model_name}.pkl\"\n",
    "    if os.path.isfile(pkl_file_name):\n",
    "        with open(pkl_file_name, 'rb') as pickle_file:\n",
    "            model = pickle.load(pickle_file)\n",
    "        model.to(device='cpu')\n",
    "        best_loss = max([x for x in model.checkpoint['best_states'].keys() if x < 4_000]) \n",
    "        model.load_state_dict(model.checkpoint['best_states'][best_loss][1])\n",
    "        loss = model.checkpoint['best_states'][best_loss][0]\n",
    "        print(f'{model_name},Epoch {best_loss}, Loss:{loss}')\n",
    "        graph_models[model_name] = model\n",
    "    else:\n",
    "        print(f'{path} not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions of baseline and graph models\n",
    "\n",
    "for i,test_set in enumerate(['Novartis', 'Literature', 'val_split']):\n",
    "\n",
    "    df_ml = test_ml_model(baseline_models, fp_data[test_set], fp_data[test_set]['y'],test_set)\n",
    "    df_gcn = test_graph_model(graph_models, loaders[test_set],test_set)\n",
    "    df= pd.concat([df_ml.drop(columns=['Dataset', 'pKa_true']),df_gcn],axis=1)\n",
    "    df= pd.concat([df_gcn],axis=1)\n",
    "    torch.cuda.empty_cache()\n",
    "    if i == 0:\n",
    "        df_res = df\n",
    "    else:\n",
    "        df_res = pd.concat([df_res,df])\n",
    "display(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= compute_stats(df_res, 'Dataset', 'pKa_true')\n",
    "test.to_csv(f'{imgdir}/stat_metrics.csv')\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot best model\n",
    "Plot the predictions of the best models for the validation and the two testsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df, x_col, y_col, hue_col = \"Dataset\"):\n",
    "    df = df.reset_index()\n",
    "    # Define plot canvas\n",
    "    g = sns.jointplot(data=df, x=x_col, y=y_col, hue=hue_col, xlim=(2,12), ylim=(2,12))\n",
    "\n",
    "    # Add fit_reg lines to plot \n",
    "    for _,gr in df.groupby(hue_col):\n",
    "        sns.regplot(x=x_col, y=y_col, data=gr, scatter=False, ax=g.ax_joint, truncate=False)\n",
    "\n",
    "    # Add Diagonal line to Joint axes\n",
    "    x0, x1 = g.ax_joint.get_xlim()\n",
    "    y0, y1 = g.ax_joint.get_ylim()\n",
    "    lims = np.array([max(x0, y0), min(x1, y1)])\n",
    "    g.ax_joint.plot(lims, lims, '-r')\n",
    "    # Add error band of pka ± 1\n",
    "    g.ax_joint.fill_between(lims, lims - 1, lims + 1, color=\"r\", alpha=0.2)\n",
    "\n",
    "    return g\n",
    "\n",
    "def calc_stat_info(y, y_hat, name):\n",
    "\n",
    "    stat_info = f\"\"\"\n",
    "        {name}\n",
    "        $r^2$ = {r2_score(y, y_hat): .2f}\n",
    "        $MAE$ = {mean_absolute_error(y, y_hat): .2f}\n",
    "        $RMSE$ = {calc_rmse(y, y_hat): .2f}\n",
    "        $kl_div$ = {compute_kl_divergence(y_hat,y): .2f}\n",
    "        \"\"\"\n",
    "    \n",
    "    return stat_info\n",
    "\n",
    "def stat_info_dict(df,x_col, y_col,datasets:list):\n",
    "    info_dict = {}\n",
    "    for name in datasets:\n",
    "        data = df[df[\"Dataset\"]==name]\n",
    "        y, y_hat = data[x_col], data[y_col]\n",
    "        info_dict[name] = calc_stat_info(y,y_hat, name)\n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper')\n",
    "set_list = [\"Novartis\", \"Literature\"]\n",
    "d = df_res[df_res[\"Dataset\"].isin(set_list)] \n",
    "# model='GCN_prot_edge'\n",
    "for model in d.columns.drop([\"pKa_true\", \"Dataset\"]):\n",
    "    # g = plot_results(d[d['Dataset']== dataset], 'pKa_true', model)\n",
    "    g = plot_results(d, 'pKa_true', model)\n",
    "    g.set_axis_labels('exp', 'pred')\n",
    "    g.fig.suptitle(f' {model}')\n",
    "    stat_info = stat_info_dict(d,'pKa_true', model, set_list)\n",
    "    g.ax_joint.text(0.25, 1, stat_info['Novartis'], size='x-small', ha='left', va=\"top\", transform = g.ax_joint.transAxes)\n",
    "    # Add stats Literature\n",
    "    g.ax_joint.text(1, 0, stat_info['Literature'], size='x-small', ha='right', va=\"bottom\", transform = g.ax_joint.transAxes)\n",
    "    plt.savefig(f'{imgdir}/regression_{model}.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCN training progress\n",
    "\n",
    "store training losses in DataFrame and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in list([\"GCNPairSingleConv\",\"GCNPairTwoConv\"]):\n",
    "    df_prog=pd.DataFrame(graph_models[m].checkpoint['progress_table'])\n",
    "#     df_prog=pd.DataFrame(graph_models_cv[x][y][1].checkpoint['progress_table'])\n",
    "    #plot learning\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(3.125,3.125)\n",
    "    sns.lineplot(x='epoch',y='train_loss', label='Train Loss',data=df_prog,ax=ax)\n",
    "    sns.lineplot(x='epoch',y='validation_loss',label='Validation Loss',data=df_prog,ax=ax)\n",
    "    ax.set_ylabel(\"Loss (MAE)\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_xlim(left=0, right=2000)\n",
    "    ax.set_ylim(top=1.75, bottom=0)\n",
    "#     plt.title(f'training progress of gcn_{x}_{y} model')\n",
    "    plt.savefig(f'{imgdir}/training_progress_gcn_{m}.pdf',bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics by tanimoto similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict with df of each test set with a similartiy column added\n",
    "def create_sim_results(df_in, datasets, sim_df=dataset_dfs, smaller_than=True, step_size=0.1):\n",
    "    results_dict = {}\n",
    "    for dataset in datasets:\n",
    "        df = df_in[df_in['Dataset']==dataset].copy()\n",
    "        df['similarity'] = sim_df[dataset].loc[:,'Similarity_max']\n",
    "        df.sort_values(by=['similarity'], inplace=True)\n",
    "\n",
    "        d = df\n",
    "        temp_dict = defaultdict(list)\n",
    "        temp_dict['similarity'] = d[\"similarity\"].unique()\n",
    "        for x in d[\"similarity\"].unique():\n",
    "            if smaller_than:\n",
    "                _d = d[d[\"similarity\"] <= x]\n",
    "            else:\n",
    "                _d = d[d[\"similarity\"] >= x]\n",
    "            for model in _d.columns.drop([\"pKa_true\", \"Dataset\", \"similarity\"]):\n",
    "                temp_dict[model].append(calc_rmse(_d['pKa_true'], _d[model]))\n",
    "        temp_df = pd.DataFrame(temp_dict)\n",
    "        temp_df[\"Dataset\"] = dataset\n",
    "        sims=[]\n",
    "        if smaller_than:\n",
    "            for sim in temp_df['similarity']:\n",
    "                x=1\n",
    "                while sim < x:\n",
    "                    x+= -step_size\n",
    "                if sim == 1:\n",
    "                    sims.append(f'= {round(np.clip(x+step_size,0,1),3)}')\n",
    "                else:\n",
    "                    sims.append(f'< {round(np.clip(x+step_size,0,1),3)}')\n",
    "        else:\n",
    "            for sim in temp_df['similarity']:\n",
    "                x=0\n",
    "                while sim > x:\n",
    "                    x+= step_size\n",
    "                if sim == 1:\n",
    "                    sims.append(f'= {round(np.clip(x+step_size,0,1),3)}')\n",
    "                else:\n",
    "                    sims.append(f'> {round(np.clip(x+step_size,0,1),3)}')\n",
    "        temp_df['group']=sims  \n",
    "        results_dict[dataset] = temp_df\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = create_sim_results(df_res, ['Novartis', 'Literature'], sim_df=dataset_dfs, smaller_than=True, step_size=0.1)\n",
    "a = pd.concat([results_dict[\"Novartis\"], results_dict[\"Literature\"]], axis=0)\n",
    "for model in df_res.columns.drop([\"pKa_true\", \"Dataset\"]):\n",
    "    sns.scatterplot(data=a, x=\"similarity\", y=model, hue=\"Dataset\")\n",
    "    plt.xlabel('Similarity')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title(model)\n",
    "    plt.savefig(f'{imgdir}/RMSE_sim_{model}.pdf')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in df_res.columns.drop([\"pKa_true\", \"Dataset\"]):\n",
    "    ax = sns.catplot(x=\"group\", y=\"GCNPairSingleConv\", hue=\"Dataset\", kind=\"point\", data=a)\n",
    "    plt.xlabel('Similarity')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title(model)\n",
    "    plt.savefig(f'{imgdir}/RMSE_sim_intervals_{model}.pdf')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Outliers top list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_molecule(m):\n",
    "  copy = Mol(m)\n",
    "  copy.Compute2DCoords(clearConfs=True)\n",
    "  return copy\n",
    "\n",
    "def group_by_range(series, range_list, decimal=1):\n",
    "    group_labels=[]\n",
    "    for x in series:\n",
    "        i=0\n",
    "        while x > range_list[i]:\n",
    "            i+=1\n",
    "        group_labels.append(round(range_list[i],decimal))\n",
    "    return group_labels \n",
    "\n",
    "data_set=['Novartis', 'Literature']\n",
    "best=[True, False]\n",
    "for data_set in data_set:\n",
    "    trues= df_res[df_res.Dataset==data_set].pKa_true\n",
    "    preds= df_res[df_res.Dataset==data_set].GCNPairTwoConv\n",
    "    diffs = []\n",
    "    errors = []\n",
    "    for pred, true in zip(preds,trues):\n",
    "        diffs.append(pred-true)\n",
    "        errors.append(abs(pred-true))\n",
    "    res = pd.concat((pd.DataFrame({'differences':diffs}),pd.DataFrame({'errors':errors}), dataset_dfs['Novartis'].loc[:,('pKa','marvin_atom','protonated', 'deprotonated', 'ID','Similarity_max')]),axis=1)\n",
    "    \n",
    "    res_e=res.loc[:, ('errors','pKa','Similarity_max')]\n",
    "    res_e['pKa']=group_by_range(res_e['pKa'],list(range(2,14,2)))\n",
    "    res_e['Similarity']=group_by_range(res['Similarity_max'],np.arange(0.0,1.2,0.2))\n",
    "    res_e=res_e.loc[:, ('errors','pKa','Similarity')]\n",
    "    res_e=res_e.groupby(['Similarity','pKa']).mean().unstack()\n",
    "#     display(res_e)\n",
    "    \n",
    "    # plt.figure(figsize=(3.125,2.5))\n",
    "    # sns.heatmap(res_e['errors'], cmap='RdYlGn_r', vmin=0,vmax=1.50)\n",
    "    # plt.savefig(f'{imgdir}/error_heatmap_{data_set}.pdf', bbox_inches='tight')\n",
    "    # plt.show()\n",
    "    \n",
    "    for mod in best:    \n",
    "        res.sort_values(by=['errors'], inplace=True, ascending=mod)\n",
    "        num=6\n",
    "        img=Draw.MolsToGridImage(res.protonated[:num].map(get_2d_molecule),\n",
    "                                 molsPerRow=3,\n",
    "                                 subImgSize=(400,350),\n",
    "                                 useSVG=True,\n",
    "                                 highlightAtomLists=[[int(i)] for i in res.marvin_atom[:num]],\n",
    "                                 legends=[f\"error:  {round(x[1],2)}, pKa:  {x[0]}, sim: {x[2]:.3f}\" for x in zip(res.pKa[:num],res.differences[:num], res.Similarity_max[:num])])\n",
    "\n",
    "        display(img)\n",
    "        name_dict={True:'best',False:'outlier'}\n",
    "        with open(f'{imgdir}/grid_{data_set}_{name_dict[mod]}.svg', 'w') as f:\n",
    "            f.write(img.data)\n",
    "    # res.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2238d854a1993a20f5a2e769920d74e68befec1db0b55829c688fae57b1385d3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pkasolver': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
