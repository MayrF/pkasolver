{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### **Imports and setup** \n",
    "Import packages, the config.py and architecture.py files and set global parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from config import (\n",
    "    LEARNING_RATE,\n",
    "    NUM_EPOCHS,\n",
    "    TRAIN_SIZE,\n",
    "    FP_BITS,\n",
    "    FP_RADIUS,\n",
    "    list_node_features,\n",
    "    list_edge_features,\n",
    "    BATCH_SIZE,\n",
    "    NUM_ESTIMATORS,\n",
    "    num_node_features,\n",
    "    num_edge_features\n",
    ")\n",
    "from pkasolver.data import (\n",
    "    load_data,\n",
    "    preprocess_all,\n",
    "    train_validation_set_split,\n",
    "    make_stat_variables,\n",
    "    make_pyg_dataset_based_on_number_of_hydrogens,\n",
    ")\n",
    "from pkasolver.ml_architecture import (\n",
    "    GCN_pair,\n",
    "    GCN_prot,\n",
    "    GCN_deprot,\n",
    "    NNConv_pair,\n",
    "    NNConv_deprot,\n",
    "    NNConv_prot,\n",
    "    gcn_full_training,\n",
    ")\n",
    "from pkasolver.chem import generate_morgan_fp_array, calculate_tanimoto_coefficient\n",
    "from pkasolver.ml import dataset_to_dataloader\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6.25, 6.25)\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "random.seed(SEED)\n",
    "imgdir = \"images_and_tables\"\n",
    "os.makedirs(imgdir, exist_ok=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Data Preprocessing**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Import raw data**\n",
    "Load data from sdf files, create conjugate molescules and store them in pandas DataFrames"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# make sure the directory for saving the run data exists\n",
    "run_filename = \"run_data2\"\n",
    "os.makedirs(f\"{run_filename}/\", exist_ok=True)\n",
    "\n",
    "# check if saved dictonary of Dataframes is available and if so, import it\n",
    "if os.path.isfile(f\"{run_filename}/data_dfs.pkl\"):\n",
    "    print(\"Loading data ...\")\n",
    "    with open(f\"{run_filename}/data_dfs.pkl\", \"rb\") as pickle_file:\n",
    "        dataset = pickle.load(pickle_file)\n",
    "\n",
    "# create DataFrames for each dataset, store them in a dictonary and save it as as .pkl file in in the run_data folder\n",
    "else:\n",
    "    print(\"Generating data ...\")\n",
    "    sdf_paths = load_data(\"../../data/Baltruschat/\")\n",
    "    dataset = preprocess_all(sdf_paths)\n",
    "    dataset[\"train_split\"], dataset[\"val_split\"] = train_validation_set_split(\n",
    "        dataset[\"Training\"], TRAIN_SIZE, SEED\n",
    "    )\n",
    "\n",
    "    with open(f\"{run_filename}/data_dfs.pkl\", \"wb\") as pickle_file:\n",
    "        pickle.dump(dataset, pickle_file)\n",
    "\n",
    "# notification\n",
    "print(dataset.keys())\n",
    "display(dataset[\"train_split\"].head(1))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generating data ...\n",
      "Training : ../../data/Baltruschat//combined_training_datasets_unique.sdf\n",
      "###############\n",
      "Novartis : ../../data/Baltruschat//novartis_cleaned_mono_unique_notraindata.sdf\n",
      "###############\n",
      "AvLiLuMoVe : ../../data/Baltruschat//AvLiLuMoVe_cleaned_mono_unique_notraindata.sdf\n",
      "###############\n",
      "dict_keys(['Training', 'Novartis', 'AvLiLuMoVe', 'train_split', 'val_split'])\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pKa</th>\n",
       "      <th>marvin_pKa</th>\n",
       "      <th>marvin_atom</th>\n",
       "      <th>marvin_pKa_type</th>\n",
       "      <th>original_dataset</th>\n",
       "      <th>ID</th>\n",
       "      <th>smiles</th>\n",
       "      <th>protonated</th>\n",
       "      <th>deprotonated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>9.7</td>\n",
       "      <td>9.63</td>\n",
       "      <td>4</td>\n",
       "      <td>basic</td>\n",
       "      <td>['chembl25']</td>\n",
       "      <td>871123</td>\n",
       "      <td>CC(C)(C)[NH2+]CC(O)c1cc(Cl)c(N)c(C(F)(F)F)c1</td>\n",
       "      <td><img data-content=\"rdkit/molecule\" src=\"data:image/png;base64,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\" alt=\"Mol\"/></td>\n",
       "      <td><img data-content=\"rdkit/molecule\" src=\"data:image/png;base64,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\" alt=\"Mol\"/></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pKa marvin_pKa marvin_atom marvin_pKa_type original_dataset      ID  \\\n",
       "536  9.7       9.63           4           basic     ['chembl25']  871123   \n",
       "\n",
       "                                           smiles  \\\n",
       "536  CC(C)(C)[NH2+]CC(O)c1cc(Cl)c(N)c(C(F)(F)F)c1   \n",
       "\n",
       "                                            protonated  \\\n",
       "536  <img data-content=\"rdkit/molecule\" src=\"data:i...   \n",
       "\n",
       "                                          deprotonated  \n",
       "536  <img data-content=\"rdkit/molecule\" src=\"data:i...  "
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Calulate fingerprint based data**\n",
    "Create Fingerprints, target-value objects and add best tanimoto similarities of fps form external validation set molecules with those of the train molecules"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check if saved dictonary of fingerprint data is available and if so, import it\n",
    "if os.path.isfile(f\"{run_filename}/fp_data.pkl\"):\n",
    "    with open(f\"{run_filename}/fp_data.pkl\", \"rb\") as pickle_file:\n",
    "        fp_data = pickle.load(pickle_file)\n",
    "\n",
    "\n",
    "# create fingerprint arrays (dim: num molecules x fp bits) for each dataset, store them in a dictonary and save it as as .pkl file in in the run_data folder\n",
    "else:\n",
    "    fp_data = {}\n",
    "    for name, df in dataset.items():\n",
    "        X_feat, y = make_stat_variables(df, [], [\"pKa\"])\n",
    "        X_prot = generate_morgan_fp_array(\n",
    "            df, \"protonated\", nBits=FP_BITS, radius=FP_RADIUS, useFeatures=True\n",
    "        )\n",
    "        X_deprot = generate_morgan_fp_array(\n",
    "            df, \"deprotonated\", nBits=FP_BITS, radius=FP_RADIUS, useFeatures=True\n",
    "        )\n",
    "        X = np.concatenate((X_prot, X_deprot), axis=1)\n",
    "        fp_data[f\"{name}\"] = {\"prot\": X_prot, \"deprot\": X_deprot, \"pair\": X, \"y\": y}\n",
    "\n",
    "    with open(f\"{run_filename}/fp_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump(fp_data, f)\n",
    "\n",
    "    # add max tanimotosimilarity to the Dataframes of external test sets\n",
    "    train_name = \"train_split\"\n",
    "    val_name = \"val_split\"\n",
    "    for name, dataset in fp_data.items():\n",
    "        if name in [train_name, val_name]:\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"calculating similarities for {name}\")\n",
    "            max_scores = []\n",
    "            for test_mol in dataset[\"prot\"]:\n",
    "                scores = []\n",
    "                for ref_mol in fp_data[train_name][\"prot\"]:\n",
    "                    scores.append(calculate_tanimoto_coefficient(test_mol, ref_mol))\n",
    "                max_scores.append(max(scores))\n",
    "            dataset[name][\"Similarity_max\"] = max_scores\n",
    "\n",
    "    with open(f\"{run_filename}/data_dfs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "# notification\n",
    "print(\"fp_data keys:\", fp_data.keys())\n",
    "print(f\"calculated/loaded fingerprint data successfully\")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Calculate graph data**\n",
    "Create graph data with node and edge features specified in the config file and prepare loaders"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# check if saved dictonary of graph data is available and if so, import it\n",
    "if os.path.isfile(f\"{run_filename}/graph_data.pkl\"):\n",
    "    print(\"Loading data ...\")\n",
    "    with open(f\"{run_filename}/graph_data.pkl\", \"rb\") as f:\n",
    "        graph_data = pickle.load(f)\n",
    "\n",
    "# create list of 'PairData' graph objects for each dataset, store them in a dictonary and save it as as .pkl file in in the run_data folder\n",
    "else:\n",
    "    print(\"Generating data ...\")\n",
    "    graph_data = {}\n",
    "    for name, df in dataset.items():\n",
    "        print(f\"Generating data for: {name}\")\n",
    "        graph_data[name] = make_pyg_dataset_based_on_number_of_hydrogens(\n",
    "            df, list_node_features, list_edge_features, paired=True\n",
    "        )\n",
    "    with open(f\"{run_filename}/graph_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump(graph_data, f)\n",
    "\n",
    "print(\"graph_data keys:\", graph_data.keys())\n",
    "\n",
    "# create an iterable loader object from the list of graph data of each dataset and store them in a dictonary\n",
    "loaders = {}\n",
    "for name, dataset in graph_data.items():\n",
    "    print(f\"Generating loader for {name}\")\n",
    "    if name == \"Training\":\n",
    "        print(\"Skipping unsplit Training dataset\")\n",
    "        continue\n",
    "    loaders[name] = dataset_to_dataloader(dataset, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# notification\n",
    "print(\"loaders keys:\", loaders.keys())\n",
    "print(f\"calculated/loaded graph data successfully\")\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generating data ...\n",
      "Generating data for: Training\n",
      "Generating data with paired boolean set to: True\n",
      "Generating data for: Novartis\n",
      "Generating data with paired boolean set to: True\n",
      "Generating data for: AvLiLuMoVe\n",
      "Generating data with paired boolean set to: True\n",
      "Generating data for: train_split\n",
      "Generating data with paired boolean set to: True\n",
      "Generating data for: val_split\n",
      "Generating data with paired boolean set to: True\n",
      "graph_data keys: dict_keys(['Training', 'Novartis', 'AvLiLuMoVe', 'train_split', 'val_split'])\n",
      "Generating loader for Training\n",
      "Skipping unsplit Training dataset\n",
      "Generating loader for Novartis\n",
      "Generating loader for AvLiLuMoVe\n",
      "Generating loader for train_split\n",
      "Generating loader for val_split\n",
      "loaders keys: dict_keys(['Novartis', 'AvLiLuMoVe', 'train_split', 'val_split'])\n",
      "calculated/loaded graph data successfully\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "next(iter(loaders['train_split']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**show feature value range**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print all possible node feature values\n",
    "values = [set() for i in range(len(list_node_features))]\n",
    "for dataset in graph_data.values():\n",
    "    for entry in dataset:\n",
    "        for i, row in enumerate(entry.x_p.cpu().T):\n",
    "            values[i] = values[i] | set(row.numpy())\n",
    "        for i, row in enumerate(entry.x_d.cpu().T):\n",
    "            values[i] = values[i] | set(row.numpy())\n",
    "print(\"Node features:\")\n",
    "for name, values in zip(list_node_features, values):\n",
    "    x = list(values)\n",
    "    x.sort()\n",
    "    print(f\"{name}:{x}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# print all possible edge feature values\n",
    "values = [set() for i in range(len(list_edge_features))]\n",
    "for dataset in graph_data.values():\n",
    "    for entry in dataset:\n",
    "        for i, row in enumerate(entry.edge_attr_p.cpu().T):\n",
    "            values[i] = values[i] | set(row.numpy())\n",
    "        for i, row in enumerate(entry.edge_attr_d.cpu().T):\n",
    "            values[i] = values[i] | set(row.numpy())\n",
    "print(\"Edge features:\")\n",
    "for name, values in zip(list_edge_features, values):\n",
    "    x = list(values)\n",
    "    x.sort()\n",
    "    print(f\"{name}:{x}\")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "next(iter(graph_data.values()))[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Training of predictive models**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **train baseline models**\n",
    "train all baseline models in protonated, deprotonated and pair mode"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "models_dict = {\n",
    "    \"RFR\": RandomForestRegressor(\n",
    "        n_estimators=NUM_ESTIMATORS, random_state=SEED\n",
    "    ),  # Baltruschat n_estimatores = 1000\n",
    "    \"PLS\": PLSRegression(),\n",
    "}\n",
    "\n",
    "baseline_models = {}\n",
    "train_name = \"train_split\"\n",
    "val_name = \"val_split\"\n",
    "\n",
    "for model_name, model_template in models_dict.items():\n",
    "    baseline_models[model_name] = {}\n",
    "    for mode, X in fp_data[train_name].items():\n",
    "        if mode == \"y\":\n",
    "            continue\n",
    "        path = f\"models/baseline/{model_name}/{mode}/\"\n",
    "        if os.path.isfile(path + \"model.pkl\"):\n",
    "            with open(path + \"model.pkl\", \"rb\") as pickle_file:\n",
    "                baseline_models[model_name][mode] = pickle.load(pickle_file)\n",
    "        else:\n",
    "            y = fp_data[train_name][\"y\"]\n",
    "            y_val = fp_data[val_name][\"y\"]\n",
    "            model = copy.deepcopy(model_template)\n",
    "            model.fit(X, y)\n",
    "            print(f\"{model_name}_{mode}: {model.score(fp_data[val_name][mode], y_val)}\")\n",
    "            baseline_models[model_name][mode] = model\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            with open(path + \"model.pkl\", \"wb\") as pickle_file:\n",
    "                pickle.dump(model, pickle_file)\n",
    "print(f\"trained/loaded baseline models successfully\")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Train graph models**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Training**\n",
    "train all graph models in protonated, deprotonated and pair mode"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "embedding_size = 96\n",
    "num_graph_layer = 4\n",
    "num_linear_layer = 2\n",
    "\n",
    "gcn_dict = {\n",
    "    \"prot\": {\"no-edge\": GCN_prot, \"edge\": NNConv_prot},\n",
    "    \"deprot\": {\"no-edge\": GCN_deprot, \"edge\": NNConv_deprot},\n",
    "    \"pair\": {\"no-edge\": GCN_pair, \"edge\": NNConv_pair},\n",
    "}\n",
    "\n",
    "mol_modes = [\"prot\", \"deprot\", \"pair\"]\n",
    "edge_modes = [\"no-edge\", \"edge\"]\n",
    "\n",
    "graph_models = {}\n",
    "for mode in mol_modes:\n",
    "    graph_models[mode] = {}\n",
    "    for edge in edge_modes:\n",
    "        path = f\"models/gcn/{mode}/{edge}/\"\n",
    "\n",
    "        if os.path.isfile(path + \"model.pkl\"):\n",
    "            with open(path + \"model.pkl\", \"rb\") as pickle_file:\n",
    "                graph_models[mode][edge] = pickle.load(pickle_file)\n",
    "            model = graph_models[mode][edge]\n",
    "        else:\n",
    "            model = gcn_dict[mode][edge](96, 4, 2, num_node_features, num_edge_features)\n",
    "            graph_models[mode][edge] = model\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        if model.checkpoint[\"epoch\"] < NUM_EPOCHS:\n",
    "            print(model.checkpoint[\"epoch\"])\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "            print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))            \n",
    "            try:\n",
    "                optimizer.load_state_dict(model.checkpoint[\"optimizer_state\"])\n",
    "            except:\n",
    "                pass\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, patience=5, verbose=True\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f'Training GCN_{mode} with {edge} at epoch {model.checkpoint[\"epoch\"]}...'\n",
    "            )\n",
    "            print(model)\n",
    "            print(f'Training on {DEVICE}.')\n",
    "            gcn_full_training(\n",
    "                model.to(device=DEVICE),\n",
    "                loaders[\"train_split\"],\n",
    "                loaders[\"val_split\"],\n",
    "                optimizer,\n",
    "                path,\n",
    "                NUM_EPOCHS,\n",
    "            )\n",
    "\n",
    "            with open(path + \"model.pkl\", \"wb\") as pickle_file:\n",
    "                pickle.dump(model, pickle_file)\n",
    "print(f\"trained/loaded gcn models successfully\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "Number of parameters:  38209\n",
      "Training GCN_prot with no-edge at epoch 0...\n",
      "GCN_prot(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(8, 96)\n",
      "    (1): GCNConv(96, 96)\n",
      "    (2): GCNConv(96, 96)\n",
      "    (3): GCNConv(96, 96)\n",
      "  )\n",
      "  (lin): ModuleList(\n",
      "    (0): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (1): Linear(in_features=96, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Training on cuda.\n",
      "Epoch: 000, Train MAE: 6.7820, Test MAE: 6.9170\n",
      "Epoch: 020, Train MAE: 1.7410, Test MAE: 1.7060\n",
      "Epoch: 040, Train MAE: 1.3170, Test MAE: 1.3100\n",
      "Epoch: 060, Train MAE: 1.0940, Test MAE: 1.1400\n",
      "Epoch: 080, Train MAE: 0.8950, Test MAE: 0.9560\n",
      "Epoch: 100, Train MAE: 0.9180, Test MAE: 0.9710\n",
      "Epoch: 120, Train MAE: 0.8330, Test MAE: 0.9120\n",
      "Epoch: 140, Train MAE: 0.8260, Test MAE: 0.9110\n",
      "Epoch: 160, Train MAE: 0.8130, Test MAE: 0.9180\n",
      "Epoch: 180, Train MAE: 0.7910, Test MAE: 0.9290\n",
      "Epoch: 200, Train MAE: 0.7530, Test MAE: 0.8700\n",
      "Epoch: 220, Train MAE: 0.7070, Test MAE: 0.8340\n",
      "Epoch: 240, Train MAE: 0.7540, Test MAE: 0.8760\n",
      "Epoch: 260, Train MAE: 0.7230, Test MAE: 0.8730\n",
      "Epoch: 280, Train MAE: 0.6330, Test MAE: 0.7950\n",
      "Epoch: 300, Train MAE: 0.6360, Test MAE: 0.8290\n",
      "Epoch: 320, Train MAE: 0.6900, Test MAE: 0.8390\n",
      "Epoch: 340, Train MAE: 0.6180, Test MAE: 0.7750\n",
      "Epoch: 360, Train MAE: 0.6260, Test MAE: 0.8140\n",
      "Epoch: 380, Train MAE: 0.5930, Test MAE: 0.7620\n",
      "Epoch: 400, Train MAE: 0.5980, Test MAE: 0.7720\n",
      "Epoch: 420, Train MAE: 0.6520, Test MAE: 0.8150\n",
      "Epoch: 440, Train MAE: 0.6150, Test MAE: 0.7850\n",
      "Epoch: 460, Train MAE: 0.6800, Test MAE: 0.8550\n",
      "Epoch: 480, Train MAE: 0.5880, Test MAE: 0.7860\n",
      "Epoch: 500, Train MAE: 0.5630, Test MAE: 0.7700\n",
      "Epoch: 520, Train MAE: 0.6210, Test MAE: 0.8410\n",
      "Epoch: 540, Train MAE: 0.5750, Test MAE: 0.7940\n",
      "Epoch: 560, Train MAE: 0.5570, Test MAE: 0.7850\n",
      "Epoch: 580, Train MAE: 0.5550, Test MAE: 0.7660\n",
      "Epoch: 600, Train MAE: 0.5850, Test MAE: 0.7960\n",
      "Epoch: 620, Train MAE: 0.5380, Test MAE: 0.7590\n",
      "Epoch: 640, Train MAE: 0.5160, Test MAE: 0.7480\n",
      "Epoch: 660, Train MAE: 0.5520, Test MAE: 0.7600\n",
      "Epoch: 680, Train MAE: 0.5800, Test MAE: 0.8070\n",
      "Epoch: 700, Train MAE: 0.5360, Test MAE: 0.7670\n",
      "Epoch: 720, Train MAE: 0.5270, Test MAE: 0.7600\n",
      "Epoch: 740, Train MAE: 0.5410, Test MAE: 0.7760\n",
      "Epoch: 760, Train MAE: 0.5520, Test MAE: 0.7720\n",
      "Epoch: 780, Train MAE: 0.5330, Test MAE: 0.7670\n",
      "Epoch: 800, Train MAE: 0.5060, Test MAE: 0.7600\n",
      "Epoch: 820, Train MAE: 0.5080, Test MAE: 0.7710\n",
      "Epoch: 840, Train MAE: 0.4840, Test MAE: 0.7160\n",
      "Epoch: 860, Train MAE: 0.5190, Test MAE: 0.7640\n",
      "Epoch: 880, Train MAE: 0.5210, Test MAE: 0.7770\n",
      "Epoch: 900, Train MAE: 0.5320, Test MAE: 0.7460\n",
      "Epoch: 920, Train MAE: 0.4840, Test MAE: 0.7460\n",
      "Epoch: 940, Train MAE: 0.5250, Test MAE: 0.7610\n",
      "Epoch: 960, Train MAE: 0.4830, Test MAE: 0.7460\n",
      "Epoch: 980, Train MAE: 0.5030, Test MAE: 0.7340\n",
      "Epoch: 1000, Train MAE: 0.5320, Test MAE: 0.7820\n",
      "Epoch: 1020, Train MAE: 0.5200, Test MAE: 0.7860\n",
      "Epoch: 1040, Train MAE: 0.5080, Test MAE: 0.7610\n",
      "Epoch: 1060, Train MAE: 0.5070, Test MAE: 0.7590\n",
      "Epoch: 1080, Train MAE: 0.4860, Test MAE: 0.7340\n",
      "Epoch: 1100, Train MAE: 0.4920, Test MAE: 0.7500\n",
      "Epoch: 1120, Train MAE: 0.5030, Test MAE: 0.7800\n",
      "Epoch: 1140, Train MAE: 0.5140, Test MAE: 0.7590\n",
      "Epoch: 1160, Train MAE: 0.4630, Test MAE: 0.7470\n",
      "Epoch: 1180, Train MAE: 0.4990, Test MAE: 0.7420\n",
      "Epoch: 1200, Train MAE: 0.5040, Test MAE: 0.7540\n",
      "Epoch: 1220, Train MAE: 0.4560, Test MAE: 0.7310\n",
      "Epoch: 1240, Train MAE: 0.4950, Test MAE: 0.7510\n",
      "Epoch: 1260, Train MAE: 0.4850, Test MAE: 0.7350\n",
      "Epoch: 1280, Train MAE: 0.4580, Test MAE: 0.7280\n",
      "Epoch: 1300, Train MAE: 0.4580, Test MAE: 0.7130\n",
      "Epoch: 1320, Train MAE: 0.5010, Test MAE: 0.7460\n",
      "Epoch: 1340, Train MAE: 0.4970, Test MAE: 0.7550\n",
      "Epoch: 1360, Train MAE: 0.4690, Test MAE: 0.7340\n",
      "Epoch: 1380, Train MAE: 0.4400, Test MAE: 0.7220\n",
      "Epoch: 1400, Train MAE: 0.4890, Test MAE: 0.7410\n",
      "Epoch: 1420, Train MAE: 0.4510, Test MAE: 0.7180\n",
      "Epoch: 1440, Train MAE: 0.4660, Test MAE: 0.7330\n",
      "Epoch: 1460, Train MAE: 0.5080, Test MAE: 0.7440\n",
      "Epoch: 1480, Train MAE: 0.4710, Test MAE: 0.7310\n",
      "Epoch: 1500, Train MAE: 0.5320, Test MAE: 0.7630\n",
      "Epoch: 1520, Train MAE: 0.4700, Test MAE: 0.7160\n",
      "Epoch: 1540, Train MAE: 0.5150, Test MAE: 0.7710\n",
      "Epoch: 1560, Train MAE: 0.5010, Test MAE: 0.7580\n",
      "Epoch: 1580, Train MAE: 0.4560, Test MAE: 0.7250\n",
      "Epoch: 1600, Train MAE: 0.4910, Test MAE: 0.7360\n",
      "Epoch: 1620, Train MAE: 0.5060, Test MAE: 0.7440\n",
      "Epoch: 1640, Train MAE: 0.4430, Test MAE: 0.7300\n",
      "Epoch: 1660, Train MAE: 0.4870, Test MAE: 0.7550\n",
      "Epoch: 1680, Train MAE: 0.5060, Test MAE: 0.7620\n",
      "Epoch: 1700, Train MAE: 0.4450, Test MAE: 0.7320\n",
      "Epoch: 1720, Train MAE: 0.4810, Test MAE: 0.7390\n",
      "Epoch: 1740, Train MAE: 0.4430, Test MAE: 0.7160\n",
      "Epoch: 1760, Train MAE: 0.4300, Test MAE: 0.7220\n",
      "Epoch: 1780, Train MAE: 0.4320, Test MAE: 0.7290\n",
      "Epoch: 1800, Train MAE: 0.4580, Test MAE: 0.7300\n",
      "Epoch: 1820, Train MAE: 0.5100, Test MAE: 0.7830\n",
      "Epoch: 1840, Train MAE: 0.4430, Test MAE: 0.7390\n",
      "Epoch: 1860, Train MAE: 0.4300, Test MAE: 0.7200\n",
      "Epoch: 1880, Train MAE: 0.4420, Test MAE: 0.7230\n",
      "Epoch: 1900, Train MAE: 0.4960, Test MAE: 0.7590\n",
      "Epoch: 1920, Train MAE: 0.4630, Test MAE: 0.7540\n",
      "Epoch: 1940, Train MAE: 0.4500, Test MAE: 0.7180\n",
      "Epoch: 1960, Train MAE: 0.4180, Test MAE: 0.7210\n",
      "Epoch: 1980, Train MAE: 0.4210, Test MAE: 0.7170\n",
      "Epoch: 2000, Train MAE: 0.4280, Test MAE: 0.7310\n",
      "0\n",
      "Number of parameters:  208065\n",
      "Training GCN_prot with edge at epoch 0...\n",
      "NNConv_prot(\n",
      "  (convs): ModuleList(\n",
      "    (0): NNConv(8, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=768, bias=True)\n",
      "    ))\n",
      "    (1): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "    (2): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "    (3): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "  )\n",
      "  (lin): ModuleList(\n",
      "    (0): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (1): Linear(in_features=96, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Training on cuda.\n",
      "Epoch: 000, Train MAE: 167.7220, Test MAE: 173.1120\n",
      "Epoch: 020, Train MAE: 1.7410, Test MAE: 1.7920\n",
      "Epoch: 040, Train MAE: 1.4790, Test MAE: 1.5490\n",
      "Epoch: 060, Train MAE: 1.3400, Test MAE: 1.4060\n",
      "Epoch: 080, Train MAE: 1.4130, Test MAE: 1.5220\n",
      "Epoch: 100, Train MAE: 1.1310, Test MAE: 1.2000\n",
      "Epoch: 120, Train MAE: 1.0070, Test MAE: 1.0840\n",
      "Epoch: 140, Train MAE: 0.9210, Test MAE: 0.9980\n",
      "Epoch: 160, Train MAE: 0.9550, Test MAE: 1.0540\n",
      "Epoch: 180, Train MAE: 0.8560, Test MAE: 0.9460\n",
      "Epoch: 200, Train MAE: 0.8150, Test MAE: 0.9170\n",
      "Epoch: 220, Train MAE: 0.6890, Test MAE: 0.8250\n",
      "Epoch: 240, Train MAE: 0.7050, Test MAE: 0.8440\n",
      "Epoch: 260, Train MAE: 0.5760, Test MAE: 0.7660\n",
      "Epoch: 280, Train MAE: 0.5520, Test MAE: 0.7720\n",
      "Epoch: 300, Train MAE: 0.5340, Test MAE: 0.7510\n",
      "Epoch: 320, Train MAE: 0.4850, Test MAE: 0.7130\n",
      "Epoch: 340, Train MAE: 0.5010, Test MAE: 0.7500\n",
      "Epoch: 360, Train MAE: 0.4230, Test MAE: 0.7050\n",
      "Epoch: 380, Train MAE: 0.4970, Test MAE: 0.7390\n",
      "Epoch: 400, Train MAE: 0.4100, Test MAE: 0.7030\n",
      "Epoch: 420, Train MAE: 0.4090, Test MAE: 0.7010\n",
      "Epoch: 440, Train MAE: 0.3400, Test MAE: 0.6330\n",
      "Epoch: 460, Train MAE: 0.3770, Test MAE: 0.6780\n",
      "Epoch: 480, Train MAE: 0.3390, Test MAE: 0.6530\n",
      "Epoch: 500, Train MAE: 0.3000, Test MAE: 0.6550\n",
      "Epoch: 520, Train MAE: 0.4090, Test MAE: 0.7040\n",
      "Epoch: 540, Train MAE: 0.2860, Test MAE: 0.6220\n",
      "Epoch: 560, Train MAE: 0.3120, Test MAE: 0.6500\n",
      "Epoch: 580, Train MAE: 0.3190, Test MAE: 0.6470\n",
      "Epoch: 600, Train MAE: 0.3010, Test MAE: 0.6550\n",
      "Epoch: 620, Train MAE: 0.2950, Test MAE: 0.6390\n",
      "Epoch: 640, Train MAE: 0.2910, Test MAE: 0.6380\n",
      "Epoch: 660, Train MAE: 0.3220, Test MAE: 0.6320\n",
      "Epoch: 680, Train MAE: 0.2720, Test MAE: 0.6200\n",
      "Epoch: 700, Train MAE: 0.2800, Test MAE: 0.6290\n",
      "Epoch: 720, Train MAE: 0.3080, Test MAE: 0.6420\n",
      "Epoch: 740, Train MAE: 0.2630, Test MAE: 0.6140\n",
      "Epoch: 760, Train MAE: 0.3150, Test MAE: 0.6560\n",
      "Epoch: 780, Train MAE: 0.2650, Test MAE: 0.6150\n",
      "Epoch: 800, Train MAE: 0.2870, Test MAE: 0.6430\n",
      "Epoch: 820, Train MAE: 0.2630, Test MAE: 0.6340\n",
      "Epoch: 840, Train MAE: 0.2530, Test MAE: 0.6370\n",
      "Epoch: 860, Train MAE: 0.2380, Test MAE: 0.6130\n",
      "Epoch: 880, Train MAE: 0.2510, Test MAE: 0.6390\n",
      "Epoch: 900, Train MAE: 0.2290, Test MAE: 0.6150\n",
      "Epoch: 920, Train MAE: 0.2330, Test MAE: 0.6170\n",
      "Epoch: 940, Train MAE: 0.2440, Test MAE: 0.6240\n",
      "Epoch: 960, Train MAE: 0.2990, Test MAE: 0.6470\n",
      "Epoch: 980, Train MAE: 0.2510, Test MAE: 0.6240\n",
      "Epoch: 1000, Train MAE: 0.2660, Test MAE: 0.6420\n",
      "Epoch: 1020, Train MAE: 0.2390, Test MAE: 0.6320\n",
      "Epoch: 1040, Train MAE: 0.2310, Test MAE: 0.6320\n",
      "Epoch: 1060, Train MAE: 0.2340, Test MAE: 0.6310\n",
      "Epoch: 1080, Train MAE: 0.2490, Test MAE: 0.6260\n",
      "Epoch: 1100, Train MAE: 0.2180, Test MAE: 0.6030\n",
      "Epoch: 1120, Train MAE: 0.2690, Test MAE: 0.6480\n",
      "Epoch: 1140, Train MAE: 0.2130, Test MAE: 0.6220\n",
      "Epoch: 1160, Train MAE: 0.2240, Test MAE: 0.6130\n",
      "Epoch: 1180, Train MAE: 0.2560, Test MAE: 0.6400\n",
      "Epoch: 1200, Train MAE: 0.2500, Test MAE: 0.6410\n",
      "Epoch: 1220, Train MAE: 0.2100, Test MAE: 0.6240\n",
      "Epoch: 1240, Train MAE: 0.2500, Test MAE: 0.6410\n",
      "Epoch: 1260, Train MAE: 0.2090, Test MAE: 0.6240\n",
      "Epoch: 1280, Train MAE: 0.2220, Test MAE: 0.6080\n",
      "Epoch: 1300, Train MAE: 0.2040, Test MAE: 0.6120\n",
      "Epoch: 1320, Train MAE: 0.2670, Test MAE: 0.6390\n",
      "Epoch: 1340, Train MAE: 0.2090, Test MAE: 0.6070\n",
      "Epoch: 1360, Train MAE: 0.2020, Test MAE: 0.5990\n",
      "Epoch: 1380, Train MAE: 0.2380, Test MAE: 0.6190\n",
      "Epoch: 1400, Train MAE: 0.2000, Test MAE: 0.5980\n",
      "Epoch: 1420, Train MAE: 0.2130, Test MAE: 0.6130\n",
      "Epoch: 1440, Train MAE: 0.2420, Test MAE: 0.6310\n",
      "Epoch: 1460, Train MAE: 0.2040, Test MAE: 0.6180\n",
      "Epoch: 1480, Train MAE: 0.2730, Test MAE: 0.6450\n",
      "Epoch: 1500, Train MAE: 0.2240, Test MAE: 0.6140\n",
      "Epoch: 1520, Train MAE: 0.2020, Test MAE: 0.6090\n",
      "Epoch: 1540, Train MAE: 0.2200, Test MAE: 0.6170\n",
      "Epoch: 1560, Train MAE: 0.1990, Test MAE: 0.6080\n",
      "Epoch: 1580, Train MAE: 0.2000, Test MAE: 0.6180\n",
      "Epoch: 1600, Train MAE: 0.2100, Test MAE: 0.6130\n",
      "Epoch: 1620, Train MAE: 0.2240, Test MAE: 0.6240\n",
      "Epoch: 1640, Train MAE: 0.2220, Test MAE: 0.6350\n",
      "Epoch: 1660, Train MAE: 0.1850, Test MAE: 0.6000\n",
      "Epoch: 1680, Train MAE: 0.2490, Test MAE: 0.6530\n",
      "Epoch: 1700, Train MAE: 0.2130, Test MAE: 0.6360\n",
      "Epoch: 1720, Train MAE: 0.2150, Test MAE: 0.6240\n",
      "Epoch: 1740, Train MAE: 0.2290, Test MAE: 0.6270\n",
      "Epoch: 1760, Train MAE: 0.2240, Test MAE: 0.6220\n",
      "Epoch: 1780, Train MAE: 0.1980, Test MAE: 0.6100\n",
      "Epoch: 1800, Train MAE: 0.2330, Test MAE: 0.6320\n",
      "Epoch: 1820, Train MAE: 0.2100, Test MAE: 0.6290\n",
      "Epoch: 1840, Train MAE: 0.1890, Test MAE: 0.6080\n",
      "Epoch: 1860, Train MAE: 0.2100, Test MAE: 0.6140\n",
      "Epoch: 1880, Train MAE: 0.2210, Test MAE: 0.6220\n",
      "Epoch: 1900, Train MAE: 0.2090, Test MAE: 0.6240\n",
      "Epoch: 1920, Train MAE: 0.2040, Test MAE: 0.6150\n",
      "Epoch: 1940, Train MAE: 0.1980, Test MAE: 0.6270\n",
      "Epoch: 1960, Train MAE: 0.2370, Test MAE: 0.6310\n",
      "Epoch: 1980, Train MAE: 0.1910, Test MAE: 0.5990\n",
      "Epoch: 2000, Train MAE: 0.2050, Test MAE: 0.6280\n",
      "0\n",
      "Number of parameters:  38209\n",
      "Training GCN_deprot with no-edge at epoch 0...\n",
      "GCN_deprot(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(8, 96)\n",
      "    (1): GCNConv(96, 96)\n",
      "    (2): GCNConv(96, 96)\n",
      "    (3): GCNConv(96, 96)\n",
      "  )\n",
      "  (lin): ModuleList(\n",
      "    (0): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (1): Linear(in_features=96, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Training on cuda.\n",
      "Epoch: 000, Train MAE: 6.7810, Test MAE: 6.9170\n",
      "Epoch: 020, Train MAE: 1.6220, Test MAE: 1.6330\n",
      "Epoch: 040, Train MAE: 1.0770, Test MAE: 1.1020\n",
      "Epoch: 060, Train MAE: 1.0280, Test MAE: 1.0650\n",
      "Epoch: 080, Train MAE: 0.9930, Test MAE: 1.0410\n",
      "Epoch: 100, Train MAE: 0.9040, Test MAE: 0.9280\n",
      "Epoch: 120, Train MAE: 0.8970, Test MAE: 0.9560\n",
      "Epoch: 140, Train MAE: 0.8550, Test MAE: 0.9240\n",
      "Epoch: 160, Train MAE: 0.8540, Test MAE: 0.9340\n",
      "Epoch: 180, Train MAE: 0.8220, Test MAE: 0.9110\n",
      "Epoch: 200, Train MAE: 0.8190, Test MAE: 0.8960\n",
      "Epoch: 220, Train MAE: 0.7960, Test MAE: 0.8800\n",
      "Epoch: 240, Train MAE: 0.7480, Test MAE: 0.8340\n",
      "Epoch: 260, Train MAE: 0.8070, Test MAE: 0.8910\n",
      "Epoch: 280, Train MAE: 0.7030, Test MAE: 0.8360\n",
      "Epoch: 300, Train MAE: 0.7440, Test MAE: 0.8560\n",
      "Epoch: 320, Train MAE: 0.6610, Test MAE: 0.7890\n",
      "Epoch: 340, Train MAE: 0.6800, Test MAE: 0.8150\n",
      "Epoch: 360, Train MAE: 0.6450, Test MAE: 0.7940\n",
      "Epoch: 380, Train MAE: 0.6240, Test MAE: 0.7740\n",
      "Epoch: 400, Train MAE: 0.6830, Test MAE: 0.8240\n",
      "Epoch: 420, Train MAE: 0.6260, Test MAE: 0.7850\n",
      "Epoch: 440, Train MAE: 0.6590, Test MAE: 0.8080\n",
      "Epoch: 460, Train MAE: 0.7220, Test MAE: 0.8890\n",
      "Epoch: 480, Train MAE: 0.5950, Test MAE: 0.7540\n",
      "Epoch: 500, Train MAE: 0.6280, Test MAE: 0.7980\n",
      "Epoch: 520, Train MAE: 0.7660, Test MAE: 0.9160\n",
      "Epoch: 540, Train MAE: 0.6570, Test MAE: 0.8230\n",
      "Epoch: 560, Train MAE: 0.5980, Test MAE: 0.7440\n",
      "Epoch: 580, Train MAE: 0.5950, Test MAE: 0.7690\n",
      "Epoch: 600, Train MAE: 0.5830, Test MAE: 0.7550\n",
      "Epoch: 620, Train MAE: 0.5840, Test MAE: 0.7640\n",
      "Epoch: 640, Train MAE: 0.6400, Test MAE: 0.8050\n",
      "Epoch: 660, Train MAE: 0.5580, Test MAE: 0.7530\n",
      "Epoch: 680, Train MAE: 0.6000, Test MAE: 0.7720\n",
      "Epoch: 700, Train MAE: 0.5950, Test MAE: 0.7910\n",
      "Epoch: 720, Train MAE: 0.5780, Test MAE: 0.7720\n",
      "Epoch: 740, Train MAE: 0.5560, Test MAE: 0.7480\n",
      "Epoch: 760, Train MAE: 0.5590, Test MAE: 0.7510\n",
      "Epoch: 780, Train MAE: 0.5270, Test MAE: 0.7420\n",
      "Epoch: 800, Train MAE: 0.5570, Test MAE: 0.7370\n",
      "Epoch: 820, Train MAE: 0.5560, Test MAE: 0.7660\n",
      "Epoch: 840, Train MAE: 0.5350, Test MAE: 0.7270\n",
      "Epoch: 860, Train MAE: 0.5180, Test MAE: 0.7180\n",
      "Epoch: 880, Train MAE: 0.5550, Test MAE: 0.7730\n",
      "Epoch: 900, Train MAE: 0.5430, Test MAE: 0.7530\n",
      "Epoch: 920, Train MAE: 0.4950, Test MAE: 0.7070\n",
      "Epoch: 940, Train MAE: 0.5100, Test MAE: 0.7370\n",
      "Epoch: 960, Train MAE: 0.5100, Test MAE: 0.7010\n",
      "Epoch: 980, Train MAE: 0.5210, Test MAE: 0.7260\n",
      "Epoch: 1000, Train MAE: 0.5200, Test MAE: 0.7250\n",
      "Epoch: 1020, Train MAE: 0.5560, Test MAE: 0.7400\n",
      "Epoch: 1040, Train MAE: 0.5140, Test MAE: 0.7230\n",
      "Epoch: 1060, Train MAE: 0.5550, Test MAE: 0.7520\n",
      "Epoch: 1080, Train MAE: 0.4960, Test MAE: 0.7240\n",
      "Epoch: 1100, Train MAE: 0.4940, Test MAE: 0.7120\n",
      "Epoch: 1120, Train MAE: 0.5280, Test MAE: 0.7380\n",
      "Epoch: 1140, Train MAE: 0.4780, Test MAE: 0.7040\n",
      "Epoch: 1160, Train MAE: 0.5190, Test MAE: 0.7280\n",
      "Epoch: 1180, Train MAE: 0.5490, Test MAE: 0.7510\n",
      "Epoch: 1200, Train MAE: 0.5080, Test MAE: 0.7040\n",
      "Epoch: 1220, Train MAE: 0.5290, Test MAE: 0.7350\n",
      "Epoch: 1240, Train MAE: 0.5270, Test MAE: 0.7220\n",
      "Epoch: 1260, Train MAE: 0.4680, Test MAE: 0.7050\n",
      "Epoch: 1280, Train MAE: 0.4970, Test MAE: 0.7210\n",
      "Epoch: 1300, Train MAE: 0.5000, Test MAE: 0.7130\n",
      "Epoch: 1320, Train MAE: 0.4870, Test MAE: 0.7270\n",
      "Epoch: 1340, Train MAE: 0.4590, Test MAE: 0.6900\n",
      "Epoch: 1360, Train MAE: 0.4430, Test MAE: 0.6710\n",
      "Epoch: 1380, Train MAE: 0.4760, Test MAE: 0.7000\n",
      "Epoch: 1400, Train MAE: 0.5030, Test MAE: 0.7410\n",
      "Epoch: 1420, Train MAE: 0.4760, Test MAE: 0.7150\n",
      "Epoch: 1440, Train MAE: 0.4680, Test MAE: 0.7110\n",
      "Epoch: 1460, Train MAE: 0.4640, Test MAE: 0.6920\n",
      "Epoch: 1480, Train MAE: 0.4750, Test MAE: 0.7080\n",
      "Epoch: 1500, Train MAE: 0.5060, Test MAE: 0.7440\n",
      "Epoch: 1520, Train MAE: 0.4580, Test MAE: 0.6960\n",
      "Epoch: 1540, Train MAE: 0.5000, Test MAE: 0.7500\n",
      "Epoch: 1560, Train MAE: 0.4790, Test MAE: 0.7170\n",
      "Epoch: 1580, Train MAE: 0.4890, Test MAE: 0.7260\n",
      "Epoch: 1600, Train MAE: 0.4710, Test MAE: 0.6830\n",
      "Epoch: 1620, Train MAE: 0.4620, Test MAE: 0.7040\n",
      "Epoch: 1640, Train MAE: 0.4790, Test MAE: 0.7250\n",
      "Epoch: 1660, Train MAE: 0.4770, Test MAE: 0.7110\n",
      "Epoch: 1680, Train MAE: 0.4340, Test MAE: 0.6770\n",
      "Epoch: 1700, Train MAE: 0.4890, Test MAE: 0.7200\n",
      "Epoch: 1720, Train MAE: 0.4470, Test MAE: 0.6890\n",
      "Epoch: 1740, Train MAE: 0.4610, Test MAE: 0.7180\n",
      "Epoch: 1760, Train MAE: 0.4810, Test MAE: 0.7210\n",
      "Epoch: 1780, Train MAE: 0.4700, Test MAE: 0.7060\n",
      "Epoch: 1800, Train MAE: 0.4510, Test MAE: 0.7150\n",
      "Epoch: 1820, Train MAE: 0.4380, Test MAE: 0.6920\n",
      "Epoch: 1840, Train MAE: 0.4420, Test MAE: 0.6880\n",
      "Epoch: 1860, Train MAE: 0.4680, Test MAE: 0.7060\n",
      "Epoch: 1880, Train MAE: 0.4810, Test MAE: 0.7370\n",
      "Epoch: 1900, Train MAE: 0.4800, Test MAE: 0.7440\n",
      "Epoch: 1920, Train MAE: 0.4640, Test MAE: 0.7110\n",
      "Epoch: 1940, Train MAE: 0.4470, Test MAE: 0.6860\n",
      "Epoch: 1960, Train MAE: 0.4930, Test MAE: 0.7150\n",
      "Epoch: 1980, Train MAE: 0.4120, Test MAE: 0.6950\n",
      "Epoch: 2000, Train MAE: 0.4260, Test MAE: 0.7010\n",
      "0\n",
      "Number of parameters:  208065\n",
      "Training GCN_deprot with edge at epoch 0...\n",
      "NNConv_deprot(\n",
      "  (convs): ModuleList(\n",
      "    (0): NNConv(8, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=768, bias=True)\n",
      "    ))\n",
      "    (1): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "    (2): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "    (3): NNConv(96, 96, aggr=\"add\", nn=Sequential(\n",
      "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=9216, bias=True)\n",
      "    ))\n",
      "  )\n",
      "  (lin): ModuleList(\n",
      "    (0): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (1): Linear(in_features=96, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Training on cuda.\n",
      "Epoch: 000, Train MAE: 169.1580, Test MAE: 174.6300\n",
      "Epoch: 020, Train MAE: 1.7650, Test MAE: 1.8050\n",
      "Epoch: 040, Train MAE: 1.5250, Test MAE: 1.6050\n",
      "Epoch: 060, Train MAE: 1.3460, Test MAE: 1.4480\n",
      "Epoch: 080, Train MAE: 1.3760, Test MAE: 1.4700\n",
      "Epoch: 100, Train MAE: 1.1220, Test MAE: 1.2010\n",
      "Epoch: 120, Train MAE: 1.0230, Test MAE: 1.1010\n",
      "Epoch: 140, Train MAE: 0.9700, Test MAE: 1.0430\n",
      "Epoch: 160, Train MAE: 0.9040, Test MAE: 0.9930\n",
      "Epoch: 180, Train MAE: 0.8770, Test MAE: 0.9630\n",
      "Epoch: 200, Train MAE: 0.7760, Test MAE: 0.8620\n",
      "Epoch: 220, Train MAE: 0.7370, Test MAE: 0.8590\n",
      "Epoch: 240, Train MAE: 0.8280, Test MAE: 0.9580\n",
      "Epoch: 260, Train MAE: 0.5940, Test MAE: 0.7610\n",
      "Epoch: 280, Train MAE: 0.6060, Test MAE: 0.7900\n",
      "Epoch: 300, Train MAE: 0.5560, Test MAE: 0.7470\n",
      "Epoch: 320, Train MAE: 0.5060, Test MAE: 0.7120\n",
      "Epoch: 340, Train MAE: 0.4720, Test MAE: 0.6850\n",
      "Epoch: 360, Train MAE: 0.4230, Test MAE: 0.6610\n",
      "Epoch: 380, Train MAE: 0.4230, Test MAE: 0.6850\n",
      "Epoch: 400, Train MAE: 0.4030, Test MAE: 0.6790\n",
      "Epoch: 420, Train MAE: 0.4290, Test MAE: 0.6770\n",
      "Epoch: 440, Train MAE: 0.3550, Test MAE: 0.6280\n",
      "Epoch: 460, Train MAE: 0.3670, Test MAE: 0.6570\n",
      "Epoch: 480, Train MAE: 0.4040, Test MAE: 0.6710\n",
      "Epoch: 500, Train MAE: 0.3210, Test MAE: 0.6300\n",
      "Epoch: 520, Train MAE: 0.3290, Test MAE: 0.6510\n",
      "Epoch: 540, Train MAE: 0.3230, Test MAE: 0.6170\n",
      "Epoch: 560, Train MAE: 0.3150, Test MAE: 0.6180\n",
      "Epoch: 580, Train MAE: 0.3030, Test MAE: 0.6210\n",
      "Epoch: 600, Train MAE: 0.3370, Test MAE: 0.6400\n",
      "Epoch: 620, Train MAE: 0.3180, Test MAE: 0.6230\n",
      "Epoch: 640, Train MAE: 0.3010, Test MAE: 0.6410\n",
      "Epoch: 660, Train MAE: 0.2920, Test MAE: 0.6070\n",
      "Epoch: 680, Train MAE: 0.2890, Test MAE: 0.6130\n",
      "Epoch: 700, Train MAE: 0.2630, Test MAE: 0.6050\n",
      "Epoch: 720, Train MAE: 0.2820, Test MAE: 0.6150\n",
      "Epoch: 740, Train MAE: 0.2740, Test MAE: 0.6080\n",
      "Epoch: 760, Train MAE: 0.2850, Test MAE: 0.6240\n",
      "Epoch: 780, Train MAE: 0.2570, Test MAE: 0.5950\n",
      "Epoch: 800, Train MAE: 0.2540, Test MAE: 0.6060\n",
      "Epoch: 820, Train MAE: 0.2650, Test MAE: 0.6160\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Cross validation**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### **prepare graph and fp data for cv**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cv_graph_data = data.slice_list(graph_data['train_split']+graph_data['val_split'],5)\n",
    "# cv_graph_train, cv_graph_val = data.cross_val_lists(cv_graph_data,run_cv)\n",
    "\n",
    "cv_loaders={'train':{},'val':{}}\n",
    "for i in range(5):\n",
    "    train, val = data.cross_val_lists(cv_graph_data,i)\n",
    "    cv_loaders['train'][i] = ml.dataset_to_dataloader(train, BATCH_SIZE)\n",
    "    cv_loaders['val'][i] = ml.dataset_to_dataloader(val, BATCH_SIZE)\n",
    "\n",
    "for name, nums in cv_loaders.items():\n",
    "    for num, obj in nums.items(): \n",
    "        print('graph_data: ',name, num, obj)\n",
    "        \n",
    "#create dictionary with modes as keys and a list of 5 arrays for each value\n",
    "cv_fp_data={}\n",
    "for name, array in fp_data['train_split'].items():\n",
    "    try:\n",
    "        cv_fp_data[name]=np.vstack((array,fp_data['val_split'][name]))\n",
    "    except:\n",
    "        cv_fp_data[name]=np.hstack((array,fp_data['val_split'][name]))\n",
    "for name, array in cv_fp_data.items():\n",
    "    cv_fp_data[name] = data.slice_list(array,5)\n",
    "\n",
    "#generate \n",
    "cv_fp_sets={'train':{},'val':{}}\n",
    "for i in range(5):\n",
    "    cv_fp_train={}\n",
    "    cv_fp_val={}\n",
    "    for name, array in cv_fp_data.items():\n",
    "        train, val = data.cross_val_lists(array,i)\n",
    "        cv_fp_train[name] = np.array(train, dtype=object)\n",
    "        cv_fp_val[name] = np.array(val, dtype=object)\n",
    "    cv_fp_sets['train'][i]=cv_fp_train\n",
    "    cv_fp_sets['val'][i]=cv_fp_val\n",
    "    \n",
    "for name, nums in cv_fp_sets.items():\n",
    "    for num, modes in nums.items():\n",
    "        print('fp_data: ',name, num, modes.keys())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### **load baseline and gcn models for cv**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mol_modes=['prot','deprot','pair']\n",
    "edge_modes=['no-edge','edge']\n",
    "cv = list(range(5))\n",
    "\n",
    "graph_models_cv = {}\n",
    "for mode in mol_modes:\n",
    "    graph_models_cv[mode] ={}\n",
    "    for edge in edge_modes:\n",
    "        graph_models_cv[mode][edge] = {}\n",
    "        for num_cv in cv: \n",
    "            path = f'cv_models/gcn/{mode}/{edge}/{num_cv}/'\n",
    "            if os.path.isfile(path+'model.pkl'):\n",
    "                with open(path+'model.pkl', 'rb') as pickle_file:\n",
    "                    graph_models_cv[mode][edge][num_cv] = pickle.load(pickle_file)\n",
    "                model = graph_models_cv[mode][edge][num_cv]\n",
    "            else:\n",
    "                print(f'cv_models/gcn/{mode}/{edge}/{num_cv}/ not found')\n",
    "\n",
    "baseline_models_cv = {}\n",
    "for name in models_dict.keys():\n",
    "    baseline_models_cv[name]={}\n",
    "    for mode in mol_modes:\n",
    "        baseline_models_cv[name][mode] ={}\n",
    "        for num_cv in cv: \n",
    "            path = f'cv_models/baseline/{name}/{mode}/{num_cv}/'\n",
    "            if os.path.isfile(path+'model.pkl'):\n",
    "                with open(path+'model.pkl', 'rb') as pickle_file:\n",
    "                    baseline_models_cv[name][mode][num_cv] = pickle.load(pickle_file)\n",
    "            else:\n",
    "                print(f'cv_models/baseline/{name}/{mode}/{num_cv}/ not found')\n",
    "                \n",
    "for name, modes in graph_models_cv.items():\n",
    "    for mode in modes.keys():\n",
    "        print('gcn: ',name, mode)\n",
    "for name, modes in baseline_models_cv.items():\n",
    "    for mode in modes.keys():\n",
    "        print('baseline: ',name, mode)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Load bestmodels**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "epoch_treshold = 2000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mol_modes=['prot','deprot','pair']\n",
    "edge_modes=['no-edge','edge']\n",
    "cv = list(range(5))\n",
    "\n",
    "graph_models_cv = {}\n",
    "for mode in mol_modes:\n",
    "    graph_models_cv[mode] ={}\n",
    "    for edge in edge_modes:\n",
    "        graph_models_cv[mode][edge] = {}\n",
    "        for num_cv in cv: \n",
    "            path = f'cv_models/gcn/{mode}/{edge}/{num_cv}/'\n",
    "            if os.path.isfile(path+'model.pkl'):\n",
    "                with open(path+'model.pkl', 'rb') as pickle_file:\n",
    "                    model = pickle.load(pickle_file)\n",
    "                best_loss = max([x for x in model.checkpoint['best_states'].keys() if x < epoch_treshold]) \n",
    "                model.load_state_dict(model.checkpoint['best_states'][best_loss][1])\n",
    "                loss = model.checkpoint['best_states'][best_loss][0]\n",
    "                print(f'GCN_{mode}_{edge}_{num_cv},Epoch {best_loss}, Loss:{loss}')\n",
    "                graph_models_cv[mode][edge][num_cv] = model\n",
    "            else:\n",
    "                print(f'{path} not found')\n",
    "                "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "graph_models = {}\n",
    "for mode in mol_modes:\n",
    "    graph_models[mode] ={}\n",
    "    for edge in edge_modes:\n",
    "        graph_models[mode][edge] = {} \n",
    "        path = f'models/gcn/{mode}/{edge}/'\n",
    "        if os.path.isfile(path+'model.pkl'):\n",
    "            with open(path+'model.pkl', 'rb') as pickle_file:\n",
    "                model = pickle.load(pickle_file)\n",
    "            best_loss = max([x for x in model.checkpoint['best_states'].keys() if x < epoch_treshold]) \n",
    "            model.load_state_dict(model.checkpoint['best_states'][best_loss][1])\n",
    "            loss = model.checkpoint['best_states'][best_loss][0]\n",
    "            print(f'GCN_{mode}_{edge},Epoch {best_loss}, Loss:{loss}')\n",
    "            graph_models[mode][edge] = model\n",
    "        else:\n",
    "            print(f'{path} not found')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Results and Analysis**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Test Baseline and Graph Models**\n",
    "test the models and the valadation and the two external sets and store their predictions and the true valus in a DataFrame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Predictions of baseline and graph models\n",
    "\n",
    "for i,test_set in enumerate(['Novartis','Literature','val_split']):\n",
    "    df_ml = ml.test_ml_model(baseline_models, fp_data[test_set], fp_data[test_set]['y'],test_set)\n",
    "    df_gcn = ml.test_graph_model(graph_models, loaders[test_set],test_set)\n",
    "    df= pd.concat([df_ml,df_gcn.drop(columns=['Dataset', 'pKa_true'])],axis=1)\n",
    "    if i == 0:\n",
    "        df_res = df\n",
    "    else:\n",
    "        df_res = pd.concat([df_res,df])\n",
    "display(df_res)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Statistical metrics**\n",
    "Calulate the Pearson Correlation Koefficient, the Root Mean Squared Error and the Mean absolute error of the validation and the two test sets for all models and list them in a DataFrame "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test= stat.compute_stats(df_res, 'Dataset', 'pKa_true')\n",
    "test.to_csv(f'{imgdir}/stat_metrics.csv')\n",
    "display(test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test= stat.compute_stats(df_res, 'Dataset', 'pKa_true')\n",
    "test.to_csv(f'{imgdir}/stat_metrics.csv')\n",
    "display(test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **CV results**\n",
    "Load models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cv_val_table= pd.concat((\n",
    "    stat.cv_graph_model(graph_models_cv,cv_loaders['val']),\n",
    "    stat.cv_ml_model(baseline_models_cv,cv_fp_sets['val'])\n",
    "))\n",
    "cv_val_table.to_csv(f'{imgdir}/cv_val_table.csv')\n",
    "display(cv_val_table)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "d ={}\n",
    "for data_set in ['Novartis','Literature']:\n",
    "    d[data_set]= pd.concat([\n",
    "    stat.cv_ml_model(baseline_models_cv,[fp_data[data_set] for i in range(5)]),\n",
    "    stat.cv_graph_model(graph_models_cv,[loaders[data_set] for i in range(5)])\n",
    "    ])\n",
    "cv_test_table=pd.concat(d.values(), axis=1, keys=d.keys())\n",
    "cv_test_table.to_csv(f'{imgdir}/cv_test_table.csv')\n",
    "cv_test_table"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Plot best model**\n",
    "Plot the predictions of the best models for the validation and the two testsets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_results(df, x_column, y_column):\n",
    "    y = df[x_column]\n",
    "    y_hat = df[y_column]\n",
    "    stat_info = f\"\"\"\n",
    "        $r^2$ = {r2_score(y, y_hat): .2f}\n",
    "        $MAE$ = {mean_absolute_error(y, y_hat): .2f}\n",
    "        $RMSE$ = {mean_squared_error(y, y_hat): .2f}\n",
    "        \"\"\"\n",
    "        # r² = 0.78 [0.74, 0.81]\n",
    "    g = sns.JointGrid(data=df, x=x_column, y=y_column, xlim=(2,12), ylim=(2,12), height=3.125)\n",
    "    g.plot_joint(sns.regplot)\n",
    "    g.plot_marginals(sns.kdeplot, shade=True)\n",
    "    g.ax_joint.text(0, 1, stat_info, size='x-small', ha='left', va=\"top\", transform = g.ax_joint.transAxes)\n",
    "    return g"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.set_context('paper')\n",
    "d = df_res\n",
    "model='GCN_pair_edge'\n",
    "for dataset in d['Dataset'].unique():\n",
    "# for dataset, model in zip(['Novartis','Literature'],['GCN_pair_edge', 'GCN_deprot_no-edge']):\n",
    "    print(dataset)\n",
    "    g = plot_results(d[d['Dataset']== dataset], 'pKa_true', model)\n",
    "    g.set_axis_labels('pKa (true)', 'gcn_pair_edge')\n",
    "    plt.savefig(f'{imgdir}/regression_{dataset}_gcn_pair_edge.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(3.125,3.125))\n",
    "    sns.residplot(data=df_res[df_res['Dataset']==dataset],x='pKa_true', y=model, lowess=True)\n",
    "    plt.ylabel('Error')\n",
    "#     plt.title('gcn_pair_edge')\n",
    "    plt.savefig(f'{imgdir}/residuals_{dataset}_gcn_pair_edge.pdf', bbox_inches='tight')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **GCN training progress**\n",
    "store training losses in DataFrame and "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for x,y in list([['pair','edge'],['prot','no-edge']]):\n",
    "    df_prog=pd.DataFrame(graph_models[x][y].checkpoint['progress_table'])\n",
    "#     df_prog=pd.DataFrame(graph_models_cv[x][y][1].checkpoint['progress_table'])\n",
    "    #plot learning\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(3.125,3.125)\n",
    "    sns.lineplot(x='epoch',y='train_loss', label='Train Loss',data=df_prog,ax=ax)\n",
    "    sns.lineplot(x='epoch',y='test_loss',label='Validation Loss',data=df_prog,ax=ax)\n",
    "    ax.set_ylabel(\"Loss (MAE)\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_xlim(left=0, right=2000)\n",
    "    ax.set_ylim(top=1.75, bottom=0)\n",
    "#     plt.title(f'training progress of gcn_{x}_{y} model')\n",
    "    plt.savefig(f'{imgdir}/training_progress_gcn_{x}_{y}.pdf',bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Feature impact**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Importances of gcn_prot_edge'**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def boxplot(attr_data):\n",
    "    plt.figure(figsize=(3.125,6.25))\n",
    "    sns.boxplot(x=\"value\", y=\"variable\",\n",
    "                orient=\"h\",\n",
    "                data=attr_data,\n",
    "                whis=[0, 100], width=.6)\n",
    "\n",
    "    # Add in points to show each observation\n",
    "    sns.stripplot(x=\"value\", y=\"variable\",\n",
    "                orient=\"h\",\n",
    "                data=attr_data,\n",
    "                size=4, color=\".3\", linewidth=0)\n",
    "    plt.ylabel('')\n",
    "\n",
    "types = ['prot','deprot','pair']\n",
    "f_modes= ['no-edge','edge']\n",
    "for data_type in types:\n",
    "    for f_mode in f_modes: \n",
    "        model = graph_models[data_type][f_mode]\n",
    "        dataset = graph_data['train_split']\n",
    "        ig = IntegratedGradients(model)\n",
    "        attr_pre_df = stat.calc_importances(ig, dataset, 100, NODE_FEATURES, EDGE_FEATURES) #adjust number of random samples\n",
    "\n",
    "        attr_pre_df.iloc[:, 1:]=attr_pre_df.iloc[:, 1:].abs()\n",
    "        attr_df=attr_pre_df.groupby('ID').max()\n",
    "        attr_data = pd.melt(attr_df)\n",
    "        \n",
    "        if data_type== 'pair':\n",
    "            split = len(attr_data.variable.unique())//2\n",
    "            attr_data1 = pd.melt(attr_df.iloc[:,0:split])\n",
    "            attr_data2 = pd.melt(attr_df.iloc[:,split:])\n",
    "        \n",
    "            boxplot(attr_data1)\n",
    "#             plt.title(f'gcn_{data_type}_1_{f_mode}')\n",
    "            plt.savefig(f'{imgdir}/importances_{data_type}_1_{f_mode}.pdf', bbox_inches='tight')\n",
    "            plt.show()\n",
    "            boxplot(attr_data2)\n",
    "#             plt.title(f'gcn_{data_type}_2_{f_mode}')\n",
    "            plt.savefig(f'{imgdir}/importances_{data_type}_2_{f_mode}.pdf', bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "            boxplot(attr_data)\n",
    "#             plt.title(f'gcn_{data_type}_{f_mode}')\n",
    "            plt.savefig(f'{imgdir}/importances_{data_type}_{f_mode}.pdf', bbox_inches='tight')\n",
    "            plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Metrics by tanimoto similarity**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x1= pd.concat([df_res[['Dataset', 'pKa_true']],df_res.loc[:,(df_res.columns.str.startswith('GCN'))]],axis=1)\n",
    "for data_set in ['Novartis', 'Literature']:\n",
    "    df = x1[x1['Dataset']==data_set].copy()\n",
    "    df['similarity'] = data_dfs[data_set].loc[:,'Similarity_max']\n",
    "    df.sort_values(by=['similarity'], inplace=True)\n",
    "\n",
    "    res=[]\n",
    "    tanimoto=[]\n",
    "    maximum=0\n",
    "\n",
    "    for i in range(2,len(df)):\n",
    "        df2 = df.iloc[:i,:]\n",
    "        new_maximum = df2['similarity'].max()\n",
    "        if new_maximum <= maximum:\n",
    "            tanimoto[-1]= new_maximum\n",
    "            res[-1]= stat.compute_stats(df2, 'Dataset', 'pKa_true',col_exclude=['similarity'])\n",
    "        else: \n",
    "            tanimoto.append(new_maximum)\n",
    "            res.append(stat.compute_stats(df2, 'Dataset', 'pKa_true', col_exclude=['similarity']))\n",
    "        maximum=new_maximum\n",
    "    result = pd.concat((res), keys=tanimoto)\n",
    "    result\n",
    "\n",
    "    # X=result['Novartis'].loc[(slice(None),'pKa_gcn_prot_edge'),:].reset_index()\n",
    "    X=result[data_set].reset_index()\n",
    "    plt.figure(figsize=(6.25,4))\n",
    "    ax = sns.scatterplot(x='level_0', y=\"RMSE\", hue='level_1', palette='colorblind', data=X)\n",
    "    legend = ax.legend()\n",
    "    legend.texts[0] = ''\n",
    "    plt.xlabel('Similarity')\n",
    "    plt.savefig(f'{imgdir}/RMSE_sim_{data_set}.pdf')\n",
    "    \n",
    "    x2= x1\n",
    "    df = x2[x2['Dataset']==data_set].copy()\n",
    "    df['similarity'] = data_dfs[data_set].loc[:,'Similarity_max']\n",
    "    df.sort_values(by=['similarity'], inplace=True)\n",
    "    \n",
    "    df = df.loc[:,('pKa_true','GCN_pair_edge','similarity')]\n",
    "\n",
    "    df['Error']= df['GCN_pair_edge']-df['pKa_true']\n",
    "\n",
    "    sims=[]\n",
    "    step_size=0.15\n",
    "    for sim in df['similarity']:\n",
    "        x=1\n",
    "        while sim < x:\n",
    "            x+= -step_size\n",
    "        sims.append(f'< {round(np.clip(x+step_size,0,1),3)}')\n",
    "    df['group']=sims            \n",
    "\n",
    "    plt.figure(figsize=(6.25/2,2.5))\n",
    "    sns.boxplot(x=\"group\", y=\"Error\",\n",
    "                orient=\"v\",\n",
    "\n",
    "                whis=[0, 100], width=.6,\n",
    "                data=df\n",
    "               )\n",
    "    # Add in points to show each observation\n",
    "    sns.stripplot(x=\"group\", y=\"Error\",\n",
    "                orient=\"v\",\n",
    "                data=df,\n",
    "                  size=4, color=\".3\", linewidth=0)\n",
    "    plt.xlabel('Similarity')\n",
    "    plt.ylabel('Error [pKa units]')\n",
    "    plt.savefig(f'{imgdir}/error_sim_bloxplot_{data_set}.pdf', bbox_inches='tight')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Outliers top list**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_2d_molecule(m):\n",
    "  copy = Chem.Mol(m)\n",
    "  copy.Compute2DCoords(clearConfs=True)\n",
    "  return copy\n",
    "\n",
    "def group_by_range(series, range_list, decimal=1):\n",
    "    group_labels=[]\n",
    "    for x in series:\n",
    "        i=0\n",
    "        while x > range_list[i]:\n",
    "            i+=1\n",
    "        group_labels.append(round(range_list[i],decimal))\n",
    "    return group_labels \n",
    "\n",
    "data_set=['Novartis', 'Literature']\n",
    "best=[True, False]\n",
    "for data_set in data_set:\n",
    "    trues= df_res[df_res.Dataset==data_set].pKa_true\n",
    "    preds= df_res[df_res.Dataset==data_set].GCN_pair_edge\n",
    "    diffs = []\n",
    "    errors = []\n",
    "    for pred, true in zip(preds,trues):\n",
    "        diffs.append(pred-true)\n",
    "        errors.append(abs(pred-true))\n",
    "    res = pd.concat((pd.DataFrame({'differences':diffs}),pd.DataFrame({'errors':errors}), data_dfs['Novartis'].loc[:,('pKa','marvin_atom','protonated', 'deprotonated', 'ID','Similarity_max')]),axis=1)\n",
    "    \n",
    "    res_e=res.loc[:, ('errors','pKa','Similarity_max')]\n",
    "    res_e['pKa']=group_by_range(res_e['pKa'],list(range(2,14,2)))\n",
    "    res_e['Similarity']=group_by_range(res['Similarity_max'],np.arange(0.0,1.2,0.2))\n",
    "    res_e=res_e.loc[:, ('errors','pKa','Similarity')]\n",
    "    res_e=res_e.groupby(['Similarity','pKa']).mean().unstack()\n",
    "#     display(res_e)\n",
    "    \n",
    "    plt.figure(figsize=(3.125,2.5))\n",
    "    sns.heatmap(res_e['errors'], cmap='RdYlGn_r', vmin=0,vmax=1.50)\n",
    "    plt.savefig(f'{imgdir}/error_heatmap_{data_set}.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    for mod in best:    \n",
    "        res.sort_values(by=['errors'], inplace=True, ascending=mod)\n",
    "        num=6\n",
    "        img=Draw.MolsToGridImage(res.protonated[:num].map(get_2d_molecule),\n",
    "                                 molsPerRow=3,\n",
    "                                 subImgSize=(400,350),\n",
    "                                 useSVG=True,\n",
    "                                 highlightAtomLists=[[int(i)] for i in res.marvin_atom[:num]],\n",
    "                                 legends=[f\"error:  {round(x[1],2)}, pKa:  {x[0]}, sim: {x[2]:.3f}\" for x in zip(res.pKa[:num],res.differences[:num], res.Similarity_max[:num])])\n",
    "\n",
    "        display(img)\n",
    "        name_dict={True:'best',False:'outlier'}\n",
    "        with open(f'{imgdir}/grid_{data_set}_{name_dict[mod]}.svg', 'w') as f:\n",
    "            f.write(img.data)\n",
    "    # res.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('pkasolver': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "1434fa83986e896e0019747847b0aafc5ed3dae518502525b9f0867561471b0e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}