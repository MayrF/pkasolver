{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, ModuleList\n",
    "from torch_geometric.nn import GCNConv, NNConv, BatchNorm, global_max_pool\n",
    "import copy\n",
    "\n",
    "# Adding GNN architectures for pka predictions\n",
    "\n",
    "def nnconv_block(embedding_size, num_graph_layer):\n",
    "    nn = Seq(Linear(num_edge_features, 16), ReLU(), Linear(16, num_node_features*embedding_size))\n",
    "    nn1 = Seq(Linear(num_edge_features, 16), ReLU(), Linear(16, embedding_size* embedding_size))\n",
    "    convs = ModuleList([NNConv(num_node_features, embedding_size, nn=nn)])\n",
    "    convs.extend([NNConv(embedding_size, embedding_size, nn=nn1) for i in range(num_graph_layer-1)])\n",
    "    return convs\n",
    "\n",
    "def gcnconv_block(embedding_size, num_graph_layer):\n",
    "    convs = ModuleList([GCNConv(num_node_features, embedding_size)])\n",
    "    convs.extend([GCNConv(embedding_size, embedding_size) for i in range(num_graph_layer-1)])\n",
    "    return convs\n",
    "\n",
    "def lin_block(embedding_size, num_linear_layer):\n",
    "    lins= ModuleList([Linear(embedding_size, embedding_size) for i in range(num_linear_layer-1)])\n",
    "    lins.extend([Linear(embedding_size, 1)])\n",
    "    return lins\n",
    "\n",
    "class GCN_single(torch.nn.Module):\n",
    "    def __init__(self, edge_conv):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(SEED)\n",
    "        \n",
    "        self.convs_x = gcnconv_block(embedding_size, num_graph_layer)\n",
    "        self.lin = lin_block(embedding_size, num_linear_layer)\n",
    "\n",
    "\n",
    "    def forward(self, x, x2, edge_attr, edge_attr2, data):\n",
    "\n",
    "        for i in range(len(self.convs_x)):\n",
    "            x = self.convs_x[i](x, data.edge_index)\n",
    "            x = x.relu()\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_max_pool(x, data.batch.to(device=DEVICE))  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        for i in range(len(self.lin)):\n",
    "                x = self.lin[i](x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "class GCN_pair(torch.nn.Module):\n",
    "    def __init__(self, edge_conv):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(SEED)\n",
    "        \n",
    "        self.convs_x = gcnconv_block(embedding_size, num_graph_layer)\n",
    "        self.convs_x2 = gcnconv_block(embedding_size, num_graph_layer)\n",
    "        self.lin = lin_block(embedding_size, num_linear_layer)     \n",
    "        \n",
    "    def forward(self, x, x2, edge_attr, edge_attr2, data):\n",
    "                        \n",
    "        for i in range(len(self.convs_x)):\n",
    "            x = self.convs_x[i](x, data.edge_index)\n",
    "            x = x.relu()\n",
    "        for i in range(len(self.convs_x2)):\n",
    "            x2 = self.convs_x2[i](x2, data.edge_index2)\n",
    "            x2 = x2.relu()\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_max_pool(x, data.batch.to(device=DEVICE))  # [batch_size, hidden_channels]\n",
    "        x2 = global_max_pool(x2, data.x2_batch.to(device=DEVICE))\n",
    "            \n",
    "        x = torch.cat((x, x2), 1)\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        for i in range(len(self.lin)):\n",
    "                x = self.lin[i](x)\n",
    "        return x\n",
    "\n",
    "class NNConv_single(torch.nn.Module):\n",
    "    def __init__(self, edge_conv):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(SEED)\n",
    "        \n",
    "        self.convs_x = nnconv_block(embedding_size, num_graph_layer)\n",
    "        self.lin = lin_block(embedding_size, num_linear_layer)   \n",
    "\n",
    "\n",
    "    def forward(self, x, x2, edge_attr, edge_attr2, data):\n",
    "\n",
    "        for i in range(len(self.convs_x)):\n",
    "            x = self.convs_x[i](x, data.edge_index, edge_attr)\n",
    "            x = x.relu()\n",
    "                \n",
    "        # 2. Readout layer\n",
    "        x = global_max_pool(x, data.batch.to(device=DEVICE))  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        for i in range(len(self.lin)):\n",
    "                x = self.lin[i](x)\n",
    "        return x\n",
    "\n",
    "class NNConv_pair(torch.nn.Module):\n",
    "    def __init__(self, edge_conv):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(SEED)\n",
    "        \n",
    "        self.convs_x = nnconv_block(embedding_size, num_graph_layer)\n",
    "        self.convs_x2 = nnconv_block(embedding_size, num_graph_layer)\n",
    "        self.lin = lin_block(embedding_size, num_linear_layer)   \n",
    "\n",
    "    def forward(self, x, x2, edge_attr, edge_attr2, data):\n",
    "    \n",
    "\n",
    "        for i in range(len(self.convs_x)):\n",
    "            x = self.convs_x[i](x, data.edge_index, edge_attr)\n",
    "            x = x.relu()\n",
    "\n",
    "        for i in range(len(self.convs_x2)):\n",
    "            x2 = self.convs_x2[i](x2, data.edge_index2, edge_attr2)\n",
    "            x2 = x2.relu()\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_max_pool(x, data.batch.to(device=DEVICE))  # [batch_size, hidden_channels] \n",
    "        x2 = global_max_pool(x2, data.x2_batch.to(device=DEVICE))\n",
    "            \n",
    "        x = torch.cat((x, x2), 1)\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        for i in range(len(self.lin)):\n",
    "                x = self.lin[i](x)\n",
    "        return x\n",
    "\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# criterion_v = torch.nn.L1Loss() # that's the MAE Loss\n",
    "# import pickle\n",
    "\n",
    "# def gcn_train(model,loader, optimizer, device='cpu'):\n",
    "#     model.train()\n",
    "#     for data in loader:  # Iterate in batches over the training dataset.\n",
    "#         data.to(device=device)\n",
    "#         out = model(x=data.x, x2=data.x2,edge_attr=data.edge_attr, edge_attr2=data.edge_attr2, data=data)\n",
    "#         loss = criterion(out.flatten(), data.y)  # Compute the loss.\n",
    "#         loss.backward()  # Derive gradients.\n",
    "#         optimizer.step()  # Update parameters based on gradients.\n",
    "#         optimizer.zero_grad() # Clear gradients.\n",
    "        \n",
    "# def gcn_test(model,loader, device='cpu'):\n",
    "#     model.eval()\n",
    "#     loss = torch.Tensor([0]).to(device=device)\n",
    "#     for data in loader:  # Iterate in batches over the training dataset.\n",
    "#         data.to(device=device)\n",
    "#         out = model(x=data.x, x2=data.x2,edge_attr=data.edge_attr, edge_attr2=data.edge_attr2, data=data)  # Perform a single forward pass.\n",
    "#         loss += criterion_v(out.flatten(), data.y)\n",
    "#     return round(float(loss/len(loader)),3) # MAE loss of batches can be summed and divided by the number of batches\n",
    "\n",
    "# def save_checkpoint(model, optimizer, epoch, train_loss, test_loss, path):\n",
    "#     point = model.checkpoint\n",
    "#     point['epoch'] = epoch + 1\n",
    "#     if point['best_loss'][0] > test_loss: \n",
    "#         point['best_loss'] = (test_loss, epoch, copy.deepcopy(model.state_dict()))\n",
    "#         point['best_states'][epoch] = (test_loss, copy.deepcopy(model.state_dict())) \n",
    "#     point['optimizer_state']=optimizer.state_dict()\n",
    "#     point['progress_table']['epoch'].append(epoch)\n",
    "#     point['progress_table']['train_loss'].append(train_loss)\n",
    "#     point['progress_table']['test_loss'].append(test_loss)\n",
    "#     with open(path+'model.pkl', 'wb') as pickle_file:\n",
    "#             pickle.dump(model,pickle_file)\n",
    "    \n",
    "\n",
    "# def gcn_full_training(model,train_loader, val_loader, optimizer, path, device='cpu'):\n",
    "#     for epoch in range(model.checkpoint['epoch'], NUM_EPOCHS+1):\n",
    "#         if epoch != 0: \n",
    "#             gcn_train(model, train_loader, optimizer, device)\n",
    "#         if epoch % 20 == 0:\n",
    "#             train_loss = gcn_test(model, train_loader, device)\n",
    "#             test_loss = gcn_test(model, val_loader, device)\n",
    "#             print(f'Epoch: {epoch:03d}, Train MAE: {train_loss:.4f}, Test MAE: {test_loss:.4f}')\n",
    "#         if epoch % 20 == 0:   #20\n",
    "#             save_checkpoint(model, optimizer, epoch, train_loss, test_loss, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
